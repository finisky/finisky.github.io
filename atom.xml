<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Finisky Garden</title>
  <icon>https://finisky.github.io/icon.png</icon>
  <subtitle>NLP, 软件工程, 产品设计</subtitle>
  <link href="https://finisky.github.io/atom.xml" rel="self"/>
  
  <link href="https://finisky.github.io/"/>
  <updated>2023-07-19T09:59:34.000Z</updated>
  <id>https://finisky.github.io/</id>
  
  <author>
    <name>finisky</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>图说文本生成解码策略</title>
    <link href="https://finisky.github.io/illustrated-decoding-strategies/"/>
    <id>https://finisky.github.io/illustrated-decoding-strategies/</id>
    <published>2023-07-19T01:46:22.000Z</published>
    <updated>2023-07-19T09:59:34.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;之前写过 &lt;a href=&quot;/nucleussampling/&quot;&gt;# Nucleus
Sampling与文本生成中的不同解码策略比较&lt;/a&gt;，不过文中缺乏图例，对于解码过程解释不够清晰，本文作为2.0版加以补充。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;对于文本生成任务，语言模型如何做到对同一个输入生成不同的输出？问题的关键在于解码策略。无论是自编码模型还是自回归模型，都是在解码阶段的每个时间步逐个生成最终文本。所谓解码，就是按照某种策略从候选词表中选择合适的词输出。除了对于模型本身的改进，不同解码策略也对文本生成质量起到重要作用。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>使LLM善假于物: Toolformer</title>
    <link href="https://finisky.github.io/toolformer-summary/"/>
    <id>https://finisky.github.io/toolformer-summary/</id>
    <published>2023-07-16T15:58:16.000Z</published>
    <updated>2023-07-16T15:59:45.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;大模型可以仅凭指令或几个示例就能解决各种新任务，但在一些看似简单的算术或搜索任务上却表现欠佳。俗话说得好，人和动物的区别就是人可以更好地使用工具。于是，Meta
AI提出了&lt;code&gt;Toolformer&lt;/code&gt;，让LLM&lt;code&gt;善假于物&lt;/code&gt;，通过自学使用外部工具。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Toolformer&lt;/code&gt;可以决定调用什么API、何时调用它们、传递什么参数及如何将API返回值融合。&lt;code&gt;Toolformer&lt;/code&gt;以自监督方式训练，每个API仅需要几个示例。它在各种下游任务中显著提升了零样本性能，而不牺牲其核心语言模型能力。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.04761&quot;&gt;# Toolformer: Language
Models Can Teach Themselves to Use Tools&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>数据为王: Textbooks Are All You Need</title>
    <link href="https://finisky.github.io/textbooks-are-all-you-need-summary/"/>
    <id>https://finisky.github.io/textbooks-are-all-you-need-summary/</id>
    <published>2023-07-09T11:48:00.000Z</published>
    <updated>2023-07-09T12:07:12.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;MSR使用“教科书”级的高质量数据训练了仅 1.3B
的面向代码任务的&lt;code&gt;phi-1&lt;/code&gt;模型，在 HumanEval 和 MBPP
上取得了很高的准确率。&lt;/p&gt;
&lt;p&gt;根据模型扩展法则，为提升模型性能，需要从增大算量和模型规模入手。这里则另辟蹊径：从数据质量出发。之前的研究证实：提升数据质量会大幅改变扩展法则趋势，能让小模型达到大模型的效果。本文则在此结论上更进一步，打破了已有的模型扩展法则，证明&lt;strong&gt;高质量的数据甚至可以在使用更少的数据和算量条件下超越大模型的SOTA&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;具体来说，用约 7B token 训练8轮，然后在少于 200M
token的数据上微调得到 1.3B 的模型 &lt;code&gt;phi-1&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.11644&quot;&gt;# Textbooks Are All You
Need&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>训练中文垂类大模型：Lawyer LLaMA</title>
    <link href="https://finisky.github.io/lawyer-llama-summary/"/>
    <id>https://finisky.github.io/lawyer-llama-summary/</id>
    <published>2023-06-19T11:20:16.000Z</published>
    <updated>2023-06-19T17:13:01.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;开源的通用能力大模型越来越多，但真正有用和落地的是在某个领域专精的垂类模型。初看上去，似乎大模型仅需要少量prompt工作就可以很好地在垂类工作，可事实并非如此。不进行领域微调的通用模型可以很快地构建80分的应用，可是大部分的实用场景，需要95甚至98分的模型效果。这也是为什么在各个领域（如金融、车载、虚拟人）大家都在训练或微调自己大模型的原因。&lt;/p&gt;
&lt;p&gt;微调这件事看上去不难，但却有很多未解问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何基于英文大模型继续训练一个中文大模型？&lt;/li&gt;
&lt;li&gt;垂类数据应该在预训练阶段引入，还是指令微调时引入？&lt;/li&gt;
&lt;li&gt;通用指令数据与垂类任务数据的混合比例有什么讲究？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的每个问题都有很多种不同的方案，但限于时间和成本，逐一实验是不可行的，AB测试也会带来额外的成本。所以有趣的事情出现了，各个玩家对自己训练时的细节都讳莫如深，自己训练的时候也都遇到过各种各样奇怪的坑。更有意思的是，即使别人提供了一些细节参考，自己在训练时未必能够复现
:-( 。&lt;/p&gt;
&lt;p&gt;代码库：&lt;a href=&quot;https://github.com/AndrewZhe/lawyer-llama&quot;&gt;# Lawyer
LLaMA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lawyer LLaMA&lt;/code&gt;技术报告：&lt;a
href=&quot;https://arxiv.org/abs/2305.15062&quot;&gt;# Lawyer LLaMA Technical
Report&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>提升大模型数学推理能力: 过程监督</title>
    <link href="https://finisky.github.io/process-supervision-reward-model/"/>
    <id>https://finisky.github.io/process-supervision-reward-model/</id>
    <published>2023-06-06T02:18:23.000Z</published>
    <updated>2023-06-06T07:43:30.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;思维链采用逐步推理的方式得到最终结果，如果模型在某一步出现幻想
(Hallucination)，则差之毫厘，谬以千里，后面的错误会被放大，导致错误结果。&lt;/p&gt;
&lt;p&gt;OpenAI最近提出使用过程监督 (Process Supervision)
减少大模型幻想并提升大模型的数学推理能力，所以什么是过程监督？&lt;/p&gt;
&lt;p&gt;过程监督 (Process Supervision) 是相对于之前的结果监督 (Outcome
Supervison)
而言。众所周知，大模型基于人工反馈的强化学习部分需要用到奖励模型 (Reward
Model, RM)，数学推理能力是基于思维链 (Chain of
Thought)。传统的奖励模型采用的是结果监督的方式，仅使用思维链的最终结果进行判别与反馈，而过程监督则是对思维链的每步推理都进行反馈。因此，过程监督是针对思维链和奖励模型的一种改进方案。&lt;/p&gt;
&lt;p&gt;OpenAI开源了过程监督的数据集
PRM800K。论文的核心思想很直观，主要关注在实验设计。&lt;/p&gt;
&lt;p&gt;OpenAI 博客：&lt;a
href=&quot;https://openai.com/research/improving-mathematical-reasoning-with-process-supervision&quot;&gt;#
Improving mathematical reasoning with process supervision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/2305.20050&quot;&gt;# Let&#39;s Verify
Step by Step&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>升级gcc解决编译llama-cpp-python错误</title>
    <link href="https://finisky.github.io/build-llama-cpp-python-error-solution/"/>
    <id>https://finisky.github.io/build-llama-cpp-python-error-solution/</id>
    <published>2023-05-11T09:18:39.000Z</published>
    <updated>2023-05-11T09:28:57.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;安装 &lt;a
href=&quot;https://github.com/oobabooga/text-generation-webui&quot;&gt;text-generation-webui&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&quot;line-numbers language-bash&quot; data-language=&quot;bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;~/text-generation-webui$ pip &lt;span class=&quot;token function&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;token parameter variable&quot;&gt;-r&lt;/span&gt; requirements.txt&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;遇到错误：&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>ERROR: Could not find a version that satisfies the requirement Solution</title>
    <link href="https://finisky.github.io/en/could-not-find-a-version-that-satisfies-the-requirement-solution/"/>
    <id>https://finisky.github.io/en/could-not-find-a-version-that-satisfies-the-requirement-solution/</id>
    <published>2023-05-10T09:05:16.000Z</published>
    <updated>2023-05-10T09:13:16.000Z</updated>
    
    
    <summary type="html">&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code class=&quot;language-none&quot;&gt;$ pip install torch&amp;#x3D;&amp;#x3D;1.12.0
Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement torch&amp;#x3D;&amp;#x3D;1.12.0 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2)
ERROR: No matching distribution found for torch&amp;#x3D;&amp;#x3D;1.12.0&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The root cause is that python version is too low (&lt;code&gt;3.6&lt;/code&gt;).
We need to upgrade python to a new version.&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="python" scheme="https://finisky.github.io/tags/python/"/>
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>贵州4月下旬自驾游记</title>
    <link href="https://finisky.github.io/guizhou/"/>
    <id>https://finisky.github.io/guizhou/</id>
    <published>2023-05-04T01:12:01.000Z</published>
    <updated>2023-05-10T09:13:16.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;好久没出去看大好河山，五一前休假去贵州自驾游，云贵川算是都打了卡。按照惯例，还是提前简单做下攻略，本想将行程设置宽松些，后来发现贵州的大部分景点门票预订（进景区时间）都要精确到小时，才不得不把行程提前细化。&lt;/p&gt;
&lt;p&gt;贵州的主要景点也比较分散，不过都是以省会贵阳为中心放射状排列，本次自驾主要以黔东为主。之前找到一张不错的贵州景点分布图，记不清出处了：&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
    <category term="自驾游" scheme="https://finisky.github.io/tags/%E8%87%AA%E9%A9%BE%E6%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>围炉对谈：OpenAI创始人对GPT-4和ChatGPT的理解</title>
    <link href="https://finisky.github.io/fireside-talk-openai-ceo-summary/"/>
    <id>https://finisky.github.io/fireside-talk-openai-ceo-summary/</id>
    <published>2023-03-30T11:02:03.000Z</published>
    <updated>2023-03-30T11:05:14.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;3月22日，NVIDIA的CEO黄仁勋与OpenAI的创始人Ilya
Sutskever进行了围炉对谈，通过视频可以更好地了解OpenAI是如何走到今天，又是如何理解ChatGPT和GPT-4这些大模型的。不过毕竟是非正式访谈，思路和观点略有发散，本文提取访谈中一些有意思的观点供参考。&lt;/p&gt;
&lt;p&gt;BTW，网上的中文完整字幕翻译对某些观点的翻译解读有误，建议看原视频。&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://blogs.nvidia.com/blog/2023/03/22/sutskever-openai-gtc/&quot;&gt;#
AI Opener: OpenAI’s Sutskever in Conversation With Jensen Huang&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>力洛克T41自行洗油保养实录</title>
    <link href="https://finisky.github.io/lelocle-maintain/"/>
    <id>https://finisky.github.io/lelocle-maintain/</id>
    <published>2023-03-05T15:49:05.000Z</published>
    <updated>2023-07-29T10:28:18.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;Update 2023-07-29&lt;/strong&gt;:
手动上弦的问题并未解决，保养后不久，问题更加严重，仅能上弦约10圈（走时约10小时），便开始不断打滑，同时发出“咔咔”的异响。如小心继续上弦，发条会突然弹松，直接停表。&lt;/p&gt;
&lt;p&gt;前后花了约一个月，买了一根发条，两个发条盒总成件，捏断两根发条内钩，最终购买一套立轮和离合轮（合轮）彻底解决问题。&lt;/p&gt;
&lt;p&gt;先买了一根新发条，换上之后问题依然存在。&lt;/p&gt;
&lt;p&gt;由于出现突然弹松的现象，怀疑发条轴与发条内钩打滑所致，感觉是上链时二者一直打滑，出现“咔咔”的异响。打开看了看发条轴，似乎有一些磨损（它的凸起不太明显，不是非常有经验的老师傅不太容易看出是否磨损），干脆买了一个新的原装发条盒。换上之后问题依旧。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>Chain-of-Thought Prompting 简读</title>
    <link href="https://finisky.github.io/chain-of-thought-prompting-summary/"/>
    <id>https://finisky.github.io/chain-of-thought-prompting-summary/</id>
    <published>2023-03-01T11:39:00.000Z</published>
    <updated>2023-03-01T11:45:49.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;语言模型越来越大，但更大的模型并没有显示出更强的计算和推理能力。去年Google提出了Chain-of-Thought
(CoT)
的方案，通过chain-of-thought提示，让模型逐步推断，使大模型的推理能力显著提升。本文来看一下chain-of-thought的原理。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;Chain-of-Thought Prompting
Elicits Reasoning in Large Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;&gt;Language
Models Perform Reasoning via Chain of Thought&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>大模型训练不稳定问题及解决方案</title>
    <link href="https://finisky.github.io/llm-training-instability-solution/"/>
    <id>https://finisky.github.io/llm-training-instability-solution/</id>
    <published>2023-02-15T02:08:12.000Z</published>
    <updated>2023-02-15T09:47:17.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;大规模语言模型的春风已经吹遍大地，大家都惊叹于大模型出色的对话能力，但是在训练大模型时遇到的训练不稳定问题(&lt;strong&gt;training
instabilities&lt;/strong&gt;)，可能关注的人并不太多。所谓量变引起质变，模型每大一个量级，就可能会出现一些意想不到的问题，比如莫名其妙的训练崩溃。当然，也有好的方面，在模型有一定规模后，是否有可能表现出一些弱智能，也很难说。&lt;/p&gt;
&lt;p&gt;言归正传，今天聊聊在训练10B以上模型时遇到的训练不稳定现象，问题原因及当前的解法。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Google拟发布ChatGPT的竞争对手Bard</title>
    <link href="https://finisky.github.io/google-open-bard-to-trusted-testers/"/>
    <id>https://finisky.github.io/google-open-bard-to-trusted-testers/</id>
    <published>2023-02-08T11:55:39.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;ChatGPT的大火让Google也坐不住了，许多人认为这一波Google已落后一个身位。坊间甚至传言创始人谢尔盖・布林都已“躬身入局”，亲自写代码了。上面的说法可以当八卦看来一乐，不过昨天微软官宣Bing和Edge浏览器要集成ChatGPT时，Google也不甘示弱，表示也要上线大模型&lt;code&gt;Bard&lt;/code&gt;
(这个名字倒也颇具浪漫主义气质：吟游诗人)。&lt;/p&gt;</summary>
    
    
    
    <category term="News" scheme="https://finisky.github.io/categories/News/"/>
    
    
  </entry>
  
  <entry>
    <title>大模型分布式训练的并行策略</title>
    <link href="https://finisky.github.io/how-to-train-large-language-model/"/>
    <id>https://finisky.github.io/how-to-train-large-language-model/</id>
    <published>2023-02-02T15:32:10.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;随着神经网络模型规模的不断增大，对硬件的显存和算力提出了新的要求。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如&lt;code&gt;Megatron-Turing NLG&lt;/code&gt;有
530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://coriva.eu.org/images/nlp/trendofnlpmodelsize.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;同时，模型是一个有机的整体，简单增加机器数量并不能提升算力，需要有并行策略和通信设计，才能实现高效的并行训练。本文简要介绍目前主流的几种并行策略：数据并行，张量并行，流水线并行和混合并行。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.11990&quot;&gt;# Using DeepSpeed and
Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative
Language Model&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://finisky.github.io/tags/Transformer/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT推出了收费版，每月20刀</title>
    <link href="https://finisky.github.io/chatgpt-plus/"/>
    <id>https://finisky.github.io/chatgpt-plus/</id>
    <published>2023-02-02T02:19:00.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;近来被人们玩坏的ChatGPT推出了收费订阅&lt;code&gt;ChatGPT Plus&lt;/code&gt;，每月20刀，提供更好的可用性，更快的回复时间，和提前试用新功能的权益。&lt;/p&gt;
&lt;p&gt;这个订阅目前仅对美国地区开放，先从之前登记的waitlist上邀请试用，后续会开放更多国家和地区。&lt;/p&gt;
&lt;p&gt;好消息是免费版继续可用，推出收费版后可以更好地服务于更多的免费用户。&lt;/p&gt;</summary>
    
    
    
    <category term="News" scheme="https://finisky.github.io/categories/News/"/>
    
    
  </entry>
  
  <entry>
    <title>Hexo Set Environment Variable</title>
    <link href="https://finisky.github.io/en/how-to-set-hexo-env/"/>
    <id>https://finisky.github.io/en/how-to-set-hexo-env/</id>
    <published>2023-01-28T03:28:52.000Z</published>
    <updated>2023-01-28T03:31:50.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;Recently I upgrade NexT theme to v8.14.1. The related post plugin
&lt;code&gt;hexo-related-popular-posts&lt;/code&gt; had been replaced by
&lt;code&gt;hexo-related-posts&lt;/code&gt;, which generates related posts by tf-idf
algorithm. However, the compute cost is a little bit heavy if you have
many posts. A good trade-off is enable this feature only for production
environment. The plugin &lt;a
href=&quot;https://github.com/sergeyzwezdin/hexo-related-posts&quot;&gt;hexo-related-posts&lt;/a&gt;
already takes this into account and use &lt;code&gt;enable_env_name&lt;/code&gt; to
disable its execution. Unfortunately, the document has typo so I takes
some time to fix it.&lt;/p&gt;
&lt;p&gt;So how to set environment variable in Hexo?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Short
Answer&lt;/strong&gt;：&lt;code&gt;$ hexo &amp;lt;command&amp;gt; --&amp;lt;env_key&amp;gt; env_value&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;The following secitons will illustrate how to enable related post on
production.&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="https://finisky.github.io/categories/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://finisky.github.io/tags/Hexo/"/>
    
    <category term="NexT" scheme="https://finisky.github.io/tags/NexT/"/>
    
  </entry>
  
  <entry>
    <title>Hexo环境变量区分生产环境</title>
    <link href="https://finisky.github.io/how-to-set-hexo-env/"/>
    <id>https://finisky.github.io/how-to-set-hexo-env/</id>
    <published>2023-01-27T16:42:27.000Z</published>
    <updated>2023-01-27T16:50:30.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;最近升级NexT主题到最新版v8.14.1，相关文章功能从v8.10开始由&lt;code&gt;hexo-related-popular-posts&lt;/code&gt;替换成了&lt;code&gt;hexo-related-posts&lt;/code&gt;，后者是用tf-idf算法对文章全文进行相似度计算而得相关文章，比&lt;code&gt;hexo-related-popular-posts&lt;/code&gt;要精准和先进一些，不过副作用是计算量变大，在文章数较多的情况下运行会比较慢，这样在写完文章后用&lt;code&gt;hexo s&lt;/code&gt;进行本地调试效率就变低了，每次文章修改都要重新计算一遍tf-idf。好在
&lt;a
href=&quot;https://github.com/sergeyzwezdin/hexo-related-posts&quot;&gt;hexo-related-posts&lt;/a&gt;
考虑到了此问题，可以通过设置&lt;code&gt;enable_env_name&lt;/code&gt;变量，只在特定环境(如生产环境)中才开启此功能。不过文档略有些问题，费了一番周折才设置环境变量成功。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;短答案&lt;/strong&gt;：&lt;code&gt;$ hexo &amp;lt;command&amp;gt; --&amp;lt;env_key&amp;gt; env_value&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;长答案：本文介绍了如何使用环境变量仅在生产环境开启相关文章功能。&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="https://finisky.github.io/categories/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://finisky.github.io/tags/Hexo/"/>
    
    <category term="NexT" scheme="https://finisky.github.io/tags/NexT/"/>
    
  </entry>
  
  <entry>
    <title>ETA 2824-2 机芯保养手册</title>
    <link href="https://finisky.github.io/eta-2824-2-manual/"/>
    <id>https://finisky.github.io/eta-2824-2-manual/</id>
    <published>2023-01-26T05:06:00.000Z</published>
    <updated>2023-01-30T07:57:08.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;ETA 2824-2
是经典的瑞士机芯之一，稳定、准确度高。网上也有一个很好的拆解点油视频：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1qJ411v7nQ/&quot;&gt;#
ETA2824机芯的保养与拆解组装过程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;不过关于2824机芯的手册百度很难搜到免费下载，在此与表友共享。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>Training Compute-Optimal Large Language Models 简读</title>
    <link href="https://finisky.github.io/training-compute-optimal-large-language-models-summary/"/>
    <id>https://finisky.github.io/training-compute-optimal-large-language-models-summary/</id>
    <published>2023-01-24T09:26:43.000Z</published>
    <updated>2023-01-24T09:57:45.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;DeepMind去年在 NeurIPS 2022
发表了一篇如何在给定计算资源条件下，用多少tokens训练最优大小的 Large
Language Models
(LLM)。之前的许多工作都仅专注于扩大模型规模，而并不增加训练数据规模，导致这些模型显著地训练不到位
(undertrained)。DeepMind训练用不同规模的数据 (从5B到500B tokens)
训练超过400个不同大小的模型 (从70M到超过16B)，发现
&lt;strong&gt;模型和训练数据规模需要同比增大&lt;/strong&gt;。根据这个假设，使用与
Gopher (280B) 同样的计算量且4倍的数据，训练了70B的最优模型
Chinchilla。它在许多下游任务上的性能显著超过了 Gopher (280B), GPT-3
(175B) Jurassic-1 (178B) 和 Megatron-Turing NLG (530B)。&lt;/p&gt;
&lt;p&gt;[NeurIPS 2022] &lt;a
href=&quot;https://openreview.net/pdf?id=iBBcRUlOAPR&quot;&gt;Training
Compute-Optimal Large Language Models&lt;/a&gt; &lt;a
href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Training Compute-Optimal Large
Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文的 Chinchilla 也是后续对话系统 &lt;a
href=&quot;/sparrow-summary/&quot;&gt;Sparrow&lt;/a&gt; 的基模型。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://finisky.github.io/tags/Transformer/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>微软与ChatGPT联手会带来什么？</title>
    <link href="https://finisky.github.io/why-microsoft-buy-chatgpt/"/>
    <id>https://finisky.github.io/why-microsoft-buy-chatgpt/</id>
    <published>2023-01-23T02:49:00.000Z</published>
    <updated>2023-01-23T10:07:16.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;最近微软投资ChatGPT的消息甚嚣尘上，二者的联手会给产业和用户带来什么？&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://www.reuters.com/technology/microsoft-talks-invest-10-bln-chatgpt-owner-semafor-2023-01-10/&quot;&gt;#
Microsoft in talks to invest $10 bln in ChatGPT-owner OpenAI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;从新闻上来看，微软会将ChatGPT集成到Office和Bing
Search。但实际情况可能不止于此，微软擅长做平台，CVP已经在&lt;a
href=&quot;https://azure.microsoft.com/en-us/blog/general-availability-of-azure-openai-service-expands-access-to-large-advanced-ai-models-with-added-enterprise-benefits/&quot;&gt;Azure
Blog&lt;/a&gt;称ChatGPT将不久在Azure OpenAI Service上可用:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Customers will also be able to access ChatGPT—a fine-tuned version of
GPT-3.5 that has been trained and runs inference on Azure AI
infrastructure—through Azure OpenAI Service soon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;好消息是这个服务可以直接让中小企业基于API研发产品而无须自行研发模型。坏消息是它的效果太好以至于自己训练的模型不能达到同水平的效果，形成对此底层服务的强依赖。&lt;/p&gt;</summary>
    
    
    
    <category term="Product" scheme="https://finisky.github.io/categories/Product/"/>
    
    
    <category term="Product" scheme="https://finisky.github.io/tags/Product/"/>
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
</feed>
