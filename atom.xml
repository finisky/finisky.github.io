<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Finisky Garden</title>
  <icon>https://finisky.github.io/icon.png</icon>
  <subtitle>NLP, 软件工程, 产品设计</subtitle>
  <link href="https://finisky.github.io/atom.xml" rel="self"/>
  
  <link href="https://finisky.github.io/"/>
  <updated>2023-07-09T11:55:44.000Z</updated>
  <id>https://finisky.github.io/</id>
  
  <author>
    <name>finisky</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据为王: Textbooks Are All You Need</title>
    <link href="https://finisky.github.io/textbooks-are-all-you-need-summary/"/>
    <id>https://finisky.github.io/textbooks-are-all-you-need-summary/</id>
    <published>2023-07-19T11:48:00.000Z</published>
    <updated>2023-07-09T11:55:44.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;MSR使用“教科书”级的高质量数据训练了仅 1.3B
的面向代码任务的&lt;code&gt;phi-1&lt;/code&gt;模型，在 HumanEval 和 MBPP
上取得了很高的准确率。&lt;/p&gt;
&lt;p&gt;根据模型扩展法则，为提升模型性能，需要从增大算量和模型规模入手。这里则另辟蹊径：从数据质量出发。之前的研究证实：提升数据质量会大幅改变扩展法则趋势，能让小模型达到大模型的效果。本文则在此结论上更进一步，打破了已有的模型扩展法则，证明&lt;strong&gt;高质量的数据甚至可以在使用更少的数据和算量条件下超越大模型的SOTA&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;具体来说，用约 7B token 训练8轮，然后在少于 200M
token的数据上微调得到 1.3B 的模型 &lt;code&gt;phi-1&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.11644&quot;&gt;# Textbooks Are All You
Need&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>训练中文垂类大模型：Lawyer LLaMA</title>
    <link href="https://finisky.github.io/lawyer-llama-summary/"/>
    <id>https://finisky.github.io/lawyer-llama-summary/</id>
    <published>2023-06-19T11:20:16.000Z</published>
    <updated>2023-06-19T17:13:01.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;开源的通用能力大模型越来越多，但真正有用和落地的是在某个领域专精的垂类模型。初看上去，似乎大模型仅需要少量prompt工作就可以很好地在垂类工作，可事实并非如此。不进行领域微调的通用模型可以很快地构建80分的应用，可是大部分的实用场景，需要95甚至98分的模型效果。这也是为什么在各个领域（如金融、车载、虚拟人）大家都在训练或微调自己大模型的原因。&lt;/p&gt;
&lt;p&gt;微调这件事看上去不难，但却有很多未解问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何基于英文大模型继续训练一个中文大模型？&lt;/li&gt;
&lt;li&gt;垂类数据应该在预训练阶段引入，还是指令微调时引入？&lt;/li&gt;
&lt;li&gt;通用指令数据与垂类任务数据的混合比例有什么讲究？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的每个问题都有很多种不同的方案，但限于时间和成本，逐一实验是不可行的，AB测试也会带来额外的成本。所以有趣的事情出现了，各个玩家对自己训练时的细节都讳莫如深，自己训练的时候也都遇到过各种各样奇怪的坑。更有意思的是，即使别人提供了一些细节参考，自己在训练时未必能够复现
:-( 。&lt;/p&gt;
&lt;p&gt;代码库：&lt;a href=&quot;https://github.com/AndrewZhe/lawyer-llama&quot;&gt;# Lawyer
LLaMA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lawyer LLaMA&lt;/code&gt;技术报告：&lt;a
href=&quot;https://arxiv.org/abs/2305.15062&quot;&gt;# Lawyer LLaMA Technical
Report&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>提升大模型数学推理能力: 过程监督</title>
    <link href="https://finisky.github.io/process-supervision-reward-model/"/>
    <id>https://finisky.github.io/process-supervision-reward-model/</id>
    <published>2023-06-06T02:18:23.000Z</published>
    <updated>2023-06-06T07:43:30.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;思维链采用逐步推理的方式得到最终结果，如果模型在某一步出现幻想
(Hallucination)，则差之毫厘，谬以千里，后面的错误会被放大，导致错误结果。&lt;/p&gt;
&lt;p&gt;OpenAI最近提出使用过程监督 (Process Supervision)
减少大模型幻想并提升大模型的数学推理能力，所以什么是过程监督？&lt;/p&gt;
&lt;p&gt;过程监督 (Process Supervision) 是相对于之前的结果监督 (Outcome
Supervison)
而言。众所周知，大模型基于人工反馈的强化学习部分需要用到奖励模型 (Reward
Model, RM)，数学推理能力是基于思维链 (Chain of
Thought)。传统的奖励模型采用的是结果监督的方式，仅使用思维链的最终结果进行判别与反馈，而过程监督则是对思维链的每步推理都进行反馈。因此，过程监督是针对思维链和奖励模型的一种改进方案。&lt;/p&gt;
&lt;p&gt;OpenAI开源了过程监督的数据集
PRM800K。论文的核心思想很直观，主要关注在实验设计。&lt;/p&gt;
&lt;p&gt;OpenAI 博客：&lt;a
href=&quot;https://openai.com/research/improving-mathematical-reasoning-with-process-supervision&quot;&gt;#
Improving mathematical reasoning with process supervision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/2305.20050&quot;&gt;# Let&#39;s Verify
Step by Step&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>升级gcc解决编译llama-cpp-python错误</title>
    <link href="https://finisky.github.io/build-llama-cpp-python-error-solution/"/>
    <id>https://finisky.github.io/build-llama-cpp-python-error-solution/</id>
    <published>2023-05-11T09:18:39.000Z</published>
    <updated>2023-05-11T09:28:57.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;安装 &lt;a
href=&quot;https://github.com/oobabooga/text-generation-webui&quot;&gt;text-generation-webui&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&quot;line-numbers language-bash&quot; data-language=&quot;bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;~/text-generation-webui$ pip &lt;span class=&quot;token function&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;token parameter variable&quot;&gt;-r&lt;/span&gt; requirements.txt&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;遇到错误：&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>ERROR: Could not find a version that satisfies the requirement Solution</title>
    <link href="https://finisky.github.io/en/could-not-find-a-version-that-satisfies-the-requirement-solution/"/>
    <id>https://finisky.github.io/en/could-not-find-a-version-that-satisfies-the-requirement-solution/</id>
    <published>2023-05-10T09:05:16.000Z</published>
    <updated>2023-05-10T09:13:16.000Z</updated>
    
    
    <summary type="html">&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code class=&quot;language-none&quot;&gt;$ pip install torch&amp;#x3D;&amp;#x3D;1.12.0
Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement torch&amp;#x3D;&amp;#x3D;1.12.0 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2)
ERROR: No matching distribution found for torch&amp;#x3D;&amp;#x3D;1.12.0&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The root cause is that python version is too low (&lt;code&gt;3.6&lt;/code&gt;).
We need to upgrade python to a new version.&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="python" scheme="https://finisky.github.io/tags/python/"/>
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>贵州4月下旬自驾游记</title>
    <link href="https://finisky.github.io/guizhou/"/>
    <id>https://finisky.github.io/guizhou/</id>
    <published>2023-05-04T01:12:01.000Z</published>
    <updated>2023-05-10T09:13:16.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;好久没出去看大好河山，五一前休假去贵州自驾游，云贵川算是都打了卡。按照惯例，还是提前简单做下攻略，本想将行程设置宽松些，后来发现贵州的大部分景点门票预订（进景区时间）都要精确到小时，才不得不把行程提前细化。&lt;/p&gt;
&lt;p&gt;贵州的主要景点也比较分散，不过都是以省会贵阳为中心放射状排列，本次自驾主要以黔东为主。之前找到一张不错的贵州景点分布图，记不清出处了：&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
    <category term="自驾游" scheme="https://finisky.github.io/tags/%E8%87%AA%E9%A9%BE%E6%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>围炉对谈：OpenAI创始人对GPT-4和ChatGPT的理解</title>
    <link href="https://finisky.github.io/fireside-talk-openai-ceo-summary/"/>
    <id>https://finisky.github.io/fireside-talk-openai-ceo-summary/</id>
    <published>2023-03-30T11:02:03.000Z</published>
    <updated>2023-03-30T11:05:14.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;3月22日，NVIDIA的CEO黄仁勋与OpenAI的创始人Ilya
Sutskever进行了围炉对谈，通过视频可以更好地了解OpenAI是如何走到今天，又是如何理解ChatGPT和GPT-4这些大模型的。不过毕竟是非正式访谈，思路和观点略有发散，本文提取访谈中一些有意思的观点供参考。&lt;/p&gt;
&lt;p&gt;BTW，网上的中文完整字幕翻译对某些观点的翻译解读有误，建议看原视频。&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://blogs.nvidia.com/blog/2023/03/22/sutskever-openai-gtc/&quot;&gt;#
AI Opener: OpenAI’s Sutskever in Conversation With Jensen Huang&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>力洛克T41自行洗油保养实录</title>
    <link href="https://finisky.github.io/lelocle-maintain/"/>
    <id>https://finisky.github.io/lelocle-maintain/</id>
    <published>2023-03-05T15:49:05.000Z</published>
    <updated>2023-03-05T16:05:46.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;戴了十几年的力洛克，一年前开始走时不准，最近每天能慢上一分钟，手动上弦似乎也有些问题，总是上不满弦，怀疑与之前疫情在家总手动上弦有关系
(最初怀疑发条断了)。&lt;/p&gt;
&lt;p&gt;距上次保养已经5年有余，天梭官方授权的店保养一次 (所谓完全服务)
约一千块，而买块新的ETA-2824-2机芯也就差不多这个价，所以再去保养显得非常不划算。老爷子年轻时玩表修表，有此家学，再加上网上有许多机芯拆解洗油点油视频，看起来也不甚困难，跃跃欲试，决定自行保养维护。&lt;/p&gt;
&lt;p&gt;前后历时一个月才保养完毕，趟坑无数。现在看来，动手时显然低估了保养洗油的难度，加之中间遇到的诸多难题，本想从玩表的过程获取些操作的成就感，不想却收获了诸多挫败感。修完后才感叹，授权店收一千块算是良心价了
:-) 。好在最终问题完美解决，记录下保养过程。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>Chain-of-Thought Prompting 简读</title>
    <link href="https://finisky.github.io/chain-of-thought-prompting-summary/"/>
    <id>https://finisky.github.io/chain-of-thought-prompting-summary/</id>
    <published>2023-03-01T11:39:00.000Z</published>
    <updated>2023-03-01T11:45:49.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;语言模型越来越大，但更大的模型并没有显示出更强的计算和推理能力。去年Google提出了Chain-of-Thought
(CoT)
的方案，通过chain-of-thought提示，让模型逐步推断，使大模型的推理能力显著提升。本文来看一下chain-of-thought的原理。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;Chain-of-Thought Prompting
Elicits Reasoning in Large Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;&gt;Language
Models Perform Reasoning via Chain of Thought&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>大模型训练不稳定问题及解决方案</title>
    <link href="https://finisky.github.io/llm-training-instability-solution/"/>
    <id>https://finisky.github.io/llm-training-instability-solution/</id>
    <published>2023-02-15T02:08:12.000Z</published>
    <updated>2023-02-15T09:47:17.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;大规模语言模型的春风已经吹遍大地，大家都惊叹于大模型出色的对话能力，但是在训练大模型时遇到的训练不稳定问题(&lt;strong&gt;training
instabilities&lt;/strong&gt;)，可能关注的人并不太多。所谓量变引起质变，模型每大一个量级，就可能会出现一些意想不到的问题，比如莫名其妙的训练崩溃。当然，也有好的方面，在模型有一定规模后，是否有可能表现出一些弱智能，也很难说。&lt;/p&gt;
&lt;p&gt;言归正传，今天聊聊在训练10B以上模型时遇到的训练不稳定现象，问题原因及当前的解法。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Google拟发布ChatGPT的竞争对手Bard</title>
    <link href="https://finisky.github.io/google-open-bard-to-trusted-testers/"/>
    <id>https://finisky.github.io/google-open-bard-to-trusted-testers/</id>
    <published>2023-02-08T11:55:39.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;ChatGPT的大火让Google也坐不住了，许多人认为这一波Google已落后一个身位。坊间甚至传言创始人谢尔盖・布林都已“躬身入局”，亲自写代码了。上面的说法可以当八卦看来一乐，不过昨天微软官宣Bing和Edge浏览器要集成ChatGPT时，Google也不甘示弱，表示也要上线大模型&lt;code&gt;Bard&lt;/code&gt;
(这个名字倒也颇具浪漫主义气质：吟游诗人)。&lt;/p&gt;</summary>
    
    
    
    <category term="News" scheme="https://finisky.github.io/categories/News/"/>
    
    
  </entry>
  
  <entry>
    <title>大模型分布式训练的并行策略</title>
    <link href="https://finisky.github.io/how-to-train-large-language-model/"/>
    <id>https://finisky.github.io/how-to-train-large-language-model/</id>
    <published>2023-02-02T15:32:10.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;随着神经网络模型规模的不断增大，对硬件的显存和算力提出了新的要求。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如&lt;code&gt;Megatron-Turing NLG&lt;/code&gt;有
530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://coriva.eu.org/images/nlp/trendofnlpmodelsize.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;同时，模型是一个有机的整体，简单增加机器数量并不能提升算力，需要有并行策略和通信设计，才能实现高效的并行训练。本文简要介绍目前主流的几种并行策略：数据并行，张量并行，流水线并行和混合并行。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.11990&quot;&gt;# Using DeepSpeed and
Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative
Language Model&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://finisky.github.io/tags/Transformer/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT推出了收费版，每月20刀</title>
    <link href="https://finisky.github.io/chatgpt-plus/"/>
    <id>https://finisky.github.io/chatgpt-plus/</id>
    <published>2023-02-02T02:19:00.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;近来被人们玩坏的ChatGPT推出了收费订阅&lt;code&gt;ChatGPT Plus&lt;/code&gt;，每月20刀，提供更好的可用性，更快的回复时间，和提前试用新功能的权益。&lt;/p&gt;
&lt;p&gt;这个订阅目前仅对美国地区开放，先从之前登记的waitlist上邀请试用，后续会开放更多国家和地区。&lt;/p&gt;
&lt;p&gt;好消息是免费版继续可用，推出收费版后可以更好地服务于更多的免费用户。&lt;/p&gt;</summary>
    
    
    
    <category term="News" scheme="https://finisky.github.io/categories/News/"/>
    
    
  </entry>
  
  <entry>
    <title>Hexo Set Environment Variable</title>
    <link href="https://finisky.github.io/en/how-to-set-hexo-env/"/>
    <id>https://finisky.github.io/en/how-to-set-hexo-env/</id>
    <published>2023-01-28T03:28:52.000Z</published>
    <updated>2023-01-28T03:31:50.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;Recently I upgrade NexT theme to v8.14.1. The related post plugin
&lt;code&gt;hexo-related-popular-posts&lt;/code&gt; had been replaced by
&lt;code&gt;hexo-related-posts&lt;/code&gt;, which generates related posts by tf-idf
algorithm. However, the compute cost is a little bit heavy if you have
many posts. A good trade-off is enable this feature only for production
environment. The plugin &lt;a
href=&quot;https://github.com/sergeyzwezdin/hexo-related-posts&quot;&gt;hexo-related-posts&lt;/a&gt;
already takes this into account and use &lt;code&gt;enable_env_name&lt;/code&gt; to
disable its execution. Unfortunately, the document has typo so I takes
some time to fix it.&lt;/p&gt;
&lt;p&gt;So how to set environment variable in Hexo?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Short
Answer&lt;/strong&gt;：&lt;code&gt;$ hexo &amp;lt;command&amp;gt; --&amp;lt;env_key&amp;gt; env_value&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;The following secitons will illustrate how to enable related post on
production.&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="https://finisky.github.io/categories/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://finisky.github.io/tags/Hexo/"/>
    
    <category term="NexT" scheme="https://finisky.github.io/tags/NexT/"/>
    
  </entry>
  
  <entry>
    <title>Hexo环境变量区分生产环境</title>
    <link href="https://finisky.github.io/how-to-set-hexo-env/"/>
    <id>https://finisky.github.io/how-to-set-hexo-env/</id>
    <published>2023-01-27T16:42:27.000Z</published>
    <updated>2023-01-27T16:50:30.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;最近升级NexT主题到最新版v8.14.1，相关文章功能从v8.10开始由&lt;code&gt;hexo-related-popular-posts&lt;/code&gt;替换成了&lt;code&gt;hexo-related-posts&lt;/code&gt;，后者是用tf-idf算法对文章全文进行相似度计算而得相关文章，比&lt;code&gt;hexo-related-popular-posts&lt;/code&gt;要精准和先进一些，不过副作用是计算量变大，在文章数较多的情况下运行会比较慢，这样在写完文章后用&lt;code&gt;hexo s&lt;/code&gt;进行本地调试效率就变低了，每次文章修改都要重新计算一遍tf-idf。好在
&lt;a
href=&quot;https://github.com/sergeyzwezdin/hexo-related-posts&quot;&gt;hexo-related-posts&lt;/a&gt;
考虑到了此问题，可以通过设置&lt;code&gt;enable_env_name&lt;/code&gt;变量，只在特定环境(如生产环境)中才开启此功能。不过文档略有些问题，费了一番周折才设置环境变量成功。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;短答案&lt;/strong&gt;：&lt;code&gt;$ hexo &amp;lt;command&amp;gt; --&amp;lt;env_key&amp;gt; env_value&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;长答案：本文介绍了如何使用环境变量仅在生产环境开启相关文章功能。&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="https://finisky.github.io/categories/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://finisky.github.io/tags/Hexo/"/>
    
    <category term="NexT" scheme="https://finisky.github.io/tags/NexT/"/>
    
  </entry>
  
  <entry>
    <title>ETA 2824-2 机芯保养手册</title>
    <link href="https://finisky.github.io/eta-2824-2-manual/"/>
    <id>https://finisky.github.io/eta-2824-2-manual/</id>
    <published>2023-01-26T05:06:00.000Z</published>
    <updated>2023-01-30T07:57:08.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;ETA 2824-2
是经典的瑞士机芯之一，稳定、准确度高。网上也有一个很好的拆解点油视频：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1qJ411v7nQ/&quot;&gt;#
ETA2824机芯的保养与拆解组装过程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;不过关于2824机芯的手册百度很难搜到免费下载，在此与表友共享。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>Training Compute-Optimal Large Language Models 简读</title>
    <link href="https://finisky.github.io/training-compute-optimal-large-language-models-summary/"/>
    <id>https://finisky.github.io/training-compute-optimal-large-language-models-summary/</id>
    <published>2023-01-24T09:26:43.000Z</published>
    <updated>2023-01-24T09:57:45.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;DeepMind去年在 NeurIPS 2022
发表了一篇如何在给定计算资源条件下，用多少tokens训练最优大小的 Large
Language Models
(LLM)。之前的许多工作都仅专注于扩大模型规模，而并不增加训练数据规模，导致这些模型显著地训练不到位
(undertrained)。DeepMind训练用不同规模的数据 (从5B到500B tokens)
训练超过400个不同大小的模型 (从70M到超过16B)，发现
&lt;strong&gt;模型和训练数据规模需要同比增大&lt;/strong&gt;。根据这个假设，使用与
Gopher (280B) 同样的计算量且4倍的数据，训练了70B的最优模型
Chinchilla。它在许多下游任务上的性能显著超过了 Gopher (280B), GPT-3
(175B) Jurassic-1 (178B) 和 Megatron-Turing NLG (530B)。&lt;/p&gt;
&lt;p&gt;[NeurIPS 2022] &lt;a
href=&quot;https://openreview.net/pdf?id=iBBcRUlOAPR&quot;&gt;Training
Compute-Optimal Large Language Models&lt;/a&gt; &lt;a
href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Training Compute-Optimal Large
Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文的 Chinchilla 也是后续对话系统 &lt;a
href=&quot;/sparrow-summary/&quot;&gt;Sparrow&lt;/a&gt; 的基模型。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://finisky.github.io/tags/Transformer/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>微软与ChatGPT联手会带来什么？</title>
    <link href="https://finisky.github.io/why-microsoft-buy-chatgpt/"/>
    <id>https://finisky.github.io/why-microsoft-buy-chatgpt/</id>
    <published>2023-01-23T02:49:00.000Z</published>
    <updated>2023-01-23T10:07:16.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;最近微软投资ChatGPT的消息甚嚣尘上，二者的联手会给产业和用户带来什么？&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://www.reuters.com/technology/microsoft-talks-invest-10-bln-chatgpt-owner-semafor-2023-01-10/&quot;&gt;#
Microsoft in talks to invest $10 bln in ChatGPT-owner OpenAI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;从新闻上来看，微软会将ChatGPT集成到Office和Bing
Search。但实际情况可能不止于此，微软擅长做平台，CVP已经在&lt;a
href=&quot;https://azure.microsoft.com/en-us/blog/general-availability-of-azure-openai-service-expands-access-to-large-advanced-ai-models-with-added-enterprise-benefits/&quot;&gt;Azure
Blog&lt;/a&gt;称ChatGPT将不久在Azure OpenAI Service上可用:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Customers will also be able to access ChatGPT—a fine-tuned version of
GPT-3.5 that has been trained and runs inference on Azure AI
infrastructure—through Azure OpenAI Service soon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;好消息是这个服务可以直接让中小企业基于API研发产品而无须自行研发模型。坏消息是它的效果太好以至于自己训练的模型不能达到同水平的效果，形成对此底层服务的强依赖。&lt;/p&gt;</summary>
    
    
    
    <category term="Product" scheme="https://finisky.github.io/categories/Product/"/>
    
    
    <category term="Product" scheme="https://finisky.github.io/tags/Product/"/>
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Git Sync Remote Branch Automatically by Webhook</title>
    <link href="https://finisky.github.io/en/git-sync-remote-branch-automatically-by-webhook/"/>
    <id>https://finisky.github.io/en/git-sync-remote-branch-automatically-by-webhook/</id>
    <published>2023-01-08T15:01:23.000Z</published>
    <updated>2023-01-24T14:30:12.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;How to make your local repository always sync with GitHub repository?
The answer is webhook.&lt;/p&gt;
&lt;p&gt;When the repo received a push event, GitHub will send a
&lt;code&gt;POST&lt;/code&gt; request to the webhook URL with details of any
subscribed events. What we need to do is to implement a webhook (on
local side) which performs &lt;code&gt;git pull&lt;/code&gt; to keep sync with
remote.&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="python" scheme="https://finisky.github.io/tags/python/"/>
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>2022 对话系统进展</title>
    <link href="https://finisky.github.io/dialog-system-progress-2022/"/>
    <id>https://finisky.github.io/dialog-system-progress-2022/</id>
    <published>2023-01-08T04:29:09.000Z</published>
    <updated>2023-02-08T06:43:06.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;2022年随着ChatGPT的大火而结束，最近一年的时间各巨头相继推出了许多表现出色的对话系统，有意思的是大家前进的方向不谋而合，不再专注模型结构和规模，而转向实用性：如何让一个对话系统更有用、更安全、更理解用户意图？&lt;/p&gt;
&lt;p&gt;对话系统在过去一年里的主要提升得益如下三点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大模型&lt;/strong&gt;：对话系统的基础，规模大才有足够的通用表示能力&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;从人工反馈学习
(RLHF)&lt;/strong&gt;：通过人工标注不同模型输出，使模型更好地与用户意图align，甚至更小的模型可达到同样效果&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;搜索API&lt;/strong&gt;：使回复有所参考，内容更具体更有用，避免胡说八道
(hallucination)&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://finisky.github.io/tags/Transformer/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
    <category term="Reinforcement Learning" scheme="https://finisky.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
</feed>
