<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Finisky Garden</title>
  <icon>https://finisky.github.io/icon.png</icon>
  <subtitle>NLP, 软件工程, 产品设计</subtitle>
  <link href="https://finisky.github.io/atom.xml" rel="self"/>
  
  <link href="https://finisky.github.io/"/>
  <updated>2024-07-21T16:11:06.000Z</updated>
  <id>https://finisky.github.io/</id>
  
  <author>
    <name>finisky</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ChatGPT擅长搜索排序吗？</title>
    <link href="https://finisky.github.io/is-chatgpt-good-at-search-ranking/"/>
    <id>https://finisky.github.io/is-chatgpt-good-at-search-ranking/</id>
    <published>2024-07-21T14:15:25.000Z</published>
    <updated>2024-07-21T16:11:06.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;大语言模型在各种与语言相关的任务中表现出了显著的零样本泛化能力，包括搜索引擎。然而，现有的工作主要利用LLM的生成能力进行信息检索，而不是直接进行段落排序。这篇EMNLP2023的论文(Outstanding
Paper)研究了LLM是否擅长搜索排序的问题。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2023.emnlp-main.923/&quot;&gt;# Is ChatGPT
Good at Search? Investigating Large Language Models as Re-Ranking
Agents&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Search" scheme="https://finisky.github.io/tags/Search/"/>
    
  </entry>
  
  <entry>
    <title>Rethinking the Role of Token Retrieval in Multi-Vector Retrieval简读</title>
    <link href="https://finisky.github.io/xtr-summary/"/>
    <id>https://finisky.github.io/xtr-summary/</id>
    <published>2024-06-30T10:58:25.000Z</published>
    <updated>2024-07-23T01:57:31.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;之前写过深度检索模型的介绍：&lt;a href=&quot;/retrievermodels/&quot;&gt;#
深度文本检索模型：DPR, PolyEncoders, DCBERT,
ColBERT&lt;/a&gt;，今天来看看DeepMind在NeurIPS
2024上的文章，对多向量检索模型（Multi-Vector
Retrieval）ColBERT进行了改进：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2304.01982&quot;&gt;Rethinking the Role of
Token Retrieval in Multi-Vector Retrieval&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;多向量检索模型由于使Query与Doc进行词元级别的交互，因此在许多信息检索基准测试中达到了SOTA。然而，其非线性评分函数无法扩展到数百万个文档，这就需要一个三阶段的推理过程：通过词元检索检索初始候选，访问所有词元向量，并对初始候选文档进行评分。非线性评分函数应用于每个候选文档的所有词元向量，使得推理过程复杂且缓慢。XTR
引入了新的目标函数，鼓励模型首先检索最重要的文档词元，对词元检索的改进使得
XTR
可以仅使用检索到的词元来对候选文档排序，而不是文档中的所有词元，因此其成本比
ColBERT 低两到三个数量级。在流行的 BEIR 基准测试中，XTR
在没有任何蒸馏的情况下，将 NDCG@10 提升了 2.8。&lt;/p&gt;
&lt;p&gt;主要改进点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;仅使用检索到的doc token而非全部doc token进行相似度计算&lt;/li&gt;
&lt;li&gt;解决了检索训练和推理之间的gap&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Search" scheme="https://finisky.github.io/tags/Search/"/>
    
  </entry>
  
  <entry>
    <title>AI搜索与大模型应用的一些思考</title>
    <link href="https://finisky.github.io/thoughts-on-ai-search-and-llm-applications/"/>
    <id>https://finisky.github.io/thoughts-on-ai-search-and-llm-applications/</id>
    <published>2024-05-10T01:28:36.000Z</published>
    <updated>2024-07-23T01:57:31.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;最近读到一篇有趣的文章，讨论了当前许多新的AI搜索产品是否会取代Google：&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://www.theverge.com/24111326/ai-search-perplexity-copilot-you-google-review&quot;&gt;Here’s
why AI search engines really can’t kill Google&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;先简要地看下这篇文章在说什么：&lt;/p&gt;
&lt;p&gt;如果要取代Google，那么这些新的产品必须可以完成Google能做的所有事情。于是，作者先收集了Top100的Google搜索查询，然后将它们输入到当前最好的一些AI搜索产品中。作者认为，虽然在某些情况下，基于LLM的搜索比一页Google搜索结果有用，但在大多数情况下，AI搜索取代Google还是相当困难的。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
    <category term="Search" scheme="https://finisky.github.io/tags/Search/"/>
    
  </entry>
  
  <entry>
    <title>WinSCP Transfer to Temporary Filename Not Working</title>
    <link href="https://finisky.github.io/en/winscp-transfer-to-temp-filename-not-working/"/>
    <id>https://finisky.github.io/en/winscp-transfer-to-temp-filename-not-working/</id>
    <published>2024-05-08T01:10:23.000Z</published>
    <updated>2024-05-08T06:17:00.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;In WinSCP, I found that when uploading file to the server, the file
doesn&#39;t have a temporary filename extension &quot;.filepart&quot;.&lt;/p&gt;
&lt;p&gt;Enable temporary filename for all files in &quot;Preferences -&amp;gt;
Transfer -&amp;gt; Endurance&quot; still does not work.&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
    <category term="Ubuntu" scheme="https://finisky.github.io/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu Server Freezes After Several Minutes</title>
    <link href="https://finisky.github.io/en/ubuntu-freezes-after-several-minutes/"/>
    <id>https://finisky.github.io/en/ubuntu-freezes-after-several-minutes/</id>
    <published>2024-04-18T04:36:16.000Z</published>
    <updated>2024-04-18T04:54:38.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;After upgrading Ubuntu 20.04 LTS to Ubuntu 22.04LTS, the server
always freezes after ~10 minutes. All services are down, cannot ssh,
connect to serial console but cannot input. However, this issue never
happen before release upgrade.&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
    <category term="Ubuntu" scheme="https://finisky.github.io/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Word Wildcard Replace Bug When Track Changes</title>
    <link href="https://finisky.github.io/en/word-wildcard-replace-bug-when-track-changes/"/>
    <id>https://finisky.github.io/en/word-wildcard-replace-bug-when-track-changes/</id>
    <published>2024-04-03T01:18:43.000Z</published>
    <updated>2024-04-05T08:33:22.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;Cannot believe that Word has such a bug: when tracking changes,
wildcard replacement cannot correctly work.&lt;/p&gt;
&lt;p&gt;I want to batch replace English parentheses with Chinese parentheses,
so I use wildcard replacement:&lt;/p&gt;
&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code class=&quot;language-none&quot;&gt;Find What: \((*)\)
Options: Use Wildcards
Replace With: （\1）&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, for &quot;(abc)&quot;, the expected result is &quot;（abc）&quot;, however,
the result is &quot;abc（）&quot;.&lt;/p&gt;</summary>
    
    
    
    <category term="Misc" scheme="https://finisky.github.io/categories/Misc/"/>
    
    
  </entry>
  
  <entry>
    <title>Word追踪修订时通配符替换Bug</title>
    <link href="https://finisky.github.io/word-wildcard-replace-bug-when-track-changes/"/>
    <id>https://finisky.github.io/word-wildcard-replace-bug-when-track-changes/</id>
    <published>2024-04-03T01:16:33.000Z</published>
    <updated>2024-04-05T08:33:22.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;没想到浓眉大眼的Office
Word居然还有这种Bug：在追踪修订时，通配符不能正确替换。&lt;/p&gt;
&lt;p&gt;在处理一个大型文档时，需要批量将英文括号替换成中文括号，因此需要使用到通配符替换：&lt;/p&gt;
&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code class=&quot;language-none&quot;&gt;Find What: \((*)\)
Options: Use Wildcards
Replace With: （\1）&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;预期行为是：将“(abc)”替换为“（abc）”，却没料到被替换成了“abc（）”。&lt;/p&gt;</summary>
    
    
    
    <category term="Misc" scheme="https://finisky.github.io/categories/Misc/"/>
    
    
  </entry>
  
  <entry>
    <title>为什么语言模型的本质是压缩器？</title>
    <link href="https://finisky.github.io/language-modeling-is-compression-summary/"/>
    <id>https://finisky.github.io/language-modeling-is-compression-summary/</id>
    <published>2024-03-26T11:05:56.000Z</published>
    <updated>2024-03-26T11:03:53.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;最早听说语言模型的本质是压缩器的想法是在&lt;a
href=&quot;/fireside-talk-openai-ceo-summary/&quot;&gt;黄仁勋和Ilya的围炉对谈&lt;/a&gt;，当时只是直觉上觉得这个说法很有意思，但却没想明白原理是什么。2023年9月，DeepMind写论文进一步论证了语言建模与压缩的等价性：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.10668&quot;&gt;# Language Modeling Is
Compression&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;长期以来，人们已经确认预测模型可以转化为无损压缩器，反之亦然。值得注意的是，近年来，机器学习领域一直专注于训练规模越来越大且功能强大的自监督语言模型。由于这些大语言模型展示了很强的预测能力，它们自然而然地也被认为是强大的压缩器。文中研究者主张通过压缩的视角来审视预测问题，并依此评估大型基座模型的压缩能力。实验证明大语言模型也是强大的通用预测器，语言模型即压缩的视角为扩展定律和上下文学习提供了新的见解。例如，Chinchilla
70B虽然主要用文本训练，但却能将ImageNet
patches和LibriSpeech样本压缩到其原始大小的43.4%和16.4%，分别超过了领域特定的压缩器，如PNG（58.5%）和FLAC（30.3%）。最后，研究者证实基于预测与压缩的等价性可以使用任何压缩器来构建条件生成模型。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文试图用简洁的语言（无公式）来说明“语言建模即压缩”的思想。原论文的思路是借助算术编码的原理和过程，然后将语言模型建模的过程与算术编码过程进行映射并证明它们等价。这个思路有些类似于NP难问题的证明：将一个问题在多项式时间归约成已知的某个NP难问题。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>大模型的涌现能力是幻象？</title>
    <link href="https://finisky.github.io/llm-emergent-ability-is-mirage-summary/"/>
    <id>https://finisky.github.io/llm-emergent-ability-is-mirage-summary/</id>
    <published>2024-01-05T17:15:46.000Z</published>
    <updated>2024-01-05T17:20:38.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;“涌现能力”可谓是大模型的神来之笔：这些能力在小规模模型中不存在，而仅在大规模模型中存在。涌现能力的神奇之处就在于两点：第一，锐利性，似乎它们瞬间从不存在变为存在；第二，不可预测性，不知道在什么规模的模型上就突现了。&lt;/p&gt;
&lt;p&gt;涌现能力相关的讨论在大模型出圈之后一直被津津乐道，尤其是在训练出的模型能力不达预期时，时常背锅：可能是模型不够大，所以不具备这样的能力。问题来了，涌现能力是否真的是大规模模型才拥有的魔法？&lt;/p&gt;
&lt;p&gt;NeurIPS 2023的Main Track Outstanding
Paper的二者之一，提出了对涌现能力的一种解释：对于特定任务和模型，在分析模型输出时，涌现能力的出现是由于研究人员选择的衡量指标所致，而非模型行为随着规模扩大而发生了根本性变化。具体而言，非线性或不连续的衡量标准会产生明显的涌现能力，而线性或连续的度量标准会导致模型性能的平滑、连续、可预测的变化。&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/&quot;&gt;#
Announcing the NeurIPS 2023 Paper Awards&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.15004&quot;&gt;# Are Emergent Abilities
of Large Language Models a Mirage?&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT一周年：开源大模型迎头赶上？</title>
    <link href="https://finisky.github.io/open-source-llm-vs-chatgpt/"/>
    <id>https://finisky.github.io/open-source-llm-vs-chatgpt/</id>
    <published>2023-12-26T01:59:45.000Z</published>
    <updated>2023-12-26T02:08:38.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;一篇有趣的综述文章，写于ChatGPT问世一周年之时，总结了开源大型语言模型（LLM）在过去一年中的发展情况。首先介绍了开源LLM的兴起，以及它们如何在各种自然语言处理任务中取得了显著的进展。然后讨论了开源LLM与闭源LLM之间的竞争，以及它们在性能和应用方面的差异。文中提到了一些具体的研究成果和进展，包括知识获取、情感分析、代码生成等。最后，论文探讨了开源LLM的未来发展方向，以及在伦理和安全方面的挑战和应对措施。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.16989&quot;&gt;# ChatGPT&#39;s One-year
Anniversary: Are Open-Source Large Language Models Catching up?&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>小模型的惊人能力: Phi-2</title>
    <link href="https://finisky.github.io/phi2-the-surprising-power-summary/"/>
    <id>https://finisky.github.io/phi2-the-surprising-power-summary/</id>
    <published>2023-12-13T10:18:16.000Z</published>
    <updated>2023-12-14T04:55:11.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;过去半年，MSR发布了一套名为&lt;code&gt;Phi&lt;/code&gt;的小模型（SLMs），取得了卓越的性能表现。其中第一个模型，1.3B
的&lt;code&gt;Phi-1&lt;/code&gt;，实现了在现有SLMs中对Python编码的最佳性能（在HumanEval和MBPP数据集上）。随后，他们将注意力扩展到常识推理和语言理解，并创建了一个新的
1.3B
模型，命名为&lt;code&gt;Phi-1.5&lt;/code&gt;，其性能相当于规模更大5倍的模型。&lt;/p&gt;
&lt;p&gt;最近MSR发布了&lt;code&gt;Phi-2&lt;/code&gt;，一个 2.7B
的语言模型，展示了卓越的推理和语言理解能力，表现出小于 13B
语言模型的最好效果。在各种测试中，&lt;code&gt;Phi-2&lt;/code&gt;与规模大达25倍的模型差不多或获胜，主要归功于模型规模和训练数据方面的创新。MSR已经在Azure
AI Studio模型目录中提供了Phi-2，以促进语言模型的研究和开发。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Phi-2&lt;/code&gt; 未放出细节的技术报告，具体可参考原博客：&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/&quot;&gt;#
Phi-2: The surprising power of small language models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第一代&lt;code&gt;Phi-1&lt;/code&gt;解读：&lt;a
href=&quot;/textbooks-are-all-you-need-summary/&quot;&gt;数据为王: Textbooks Are All
You Need&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Skywork: A More Open Bilingual Foundation Model 简读</title>
    <link href="https://finisky.github.io/skywork-summary/"/>
    <id>https://finisky.github.io/skywork-summary/</id>
    <published>2023-11-10T11:18:16.000Z</published>
    <updated>2023-11-11T02:29:44.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;昆仑的天工模型一直走开源路线，最近放出了技术报告，其中关于预训练模型刷榜作弊的部分引发了广泛的讨论，把大家心照不宣的事情首次放到了台面上
:-)。本文来看下这篇技术报告的亮点(非全文精读，仅摘要有趣的点，细节可阅读原论文)。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.19341&quot;&gt;# Skywork: A More Open
Bilingual Foundation Model&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Windows Explorer Crashes on Right Click Menu Solution</title>
    <link href="https://finisky.github.io/en/explorer-crash-fix/"/>
    <id>https://finisky.github.io/en/explorer-crash-fix/</id>
    <published>2023-10-03T11:20:29.000Z</published>
    <updated>2023-10-03T11:21:29.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;Recently, when I attempt to open the property window of a folder in
File Explorer, it crashes all the time. It bothers me a lot and I
finally fix it by cleaning up the right click context menu.&lt;/p&gt;</summary>
    
    
    
    <category term="Misc" scheme="https://finisky.github.io/categories/Misc/"/>
    
    
  </entry>
  
  <entry>
    <title>十一前洛阳自驾游记</title>
    <link href="https://finisky.github.io/luoyang/"/>
    <id>https://finisky.github.io/luoyang/</id>
    <published>2023-09-28T09:49:43.000Z</published>
    <updated>2023-09-28T09:50:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;趁着十一前的假期，去神都洛阳逛逛。作为一个河南人，河南的景点却基本没怎么去过，这种感觉有点类似住的最近的人常常会迟到一样。&lt;/p&gt;
&lt;p&gt;没有太多规划，原计划是龙门石窟、白马寺和少林寺，不过后来因为一直下雨和略感疲倦，逛完白马寺直接回家。自驾往返共400公里整，大部分是高速，剩下几十公里国道。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
    <category term="自驾游" scheme="https://finisky.github.io/tags/%E8%87%AA%E9%A9%BE%E6%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>LIMA: Less Is More for Alignment 简读</title>
    <link href="https://finisky.github.io/lima-summary/"/>
    <id>https://finisky.github.io/lima-summary/</id>
    <published>2023-09-12T11:18:16.000Z</published>
    <updated>2023-09-12T13:00:25.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;LIMA是五月份的一篇网红文，发表后引起了广泛的讨论。它用极致简约的SFT方案训练了一个不错的模型，希望证实表面对齐假设
(Superficial Alignment Hypothesis)：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;大模型中几乎所有知识都是在预训练中学习的，指令微调只是一个很简单的过程，让模型学到与用户交互的形式。&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;为证明上述假设，作者们推测仅需要少量的指令微调数据(1000条)就可以教会模型产生高质量输出。此外，1000条SFT数据就达到了很好的指令微调效果，也说明了高质量数据对于模型的重要性，这一点与
&lt;a href=&quot;/textbooks-are-all-you-need-summary/&quot;&gt;Textbooks Are All You
Need&lt;/a&gt; 有异曲同工之妙。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.11206&quot;&gt;# LIMA: Less Is More for
Alignment&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>力洛克T41发条突然弹松问题</title>
    <link href="https://finisky.github.io/lelocle-mainspring-suddenly-release/"/>
    <id>https://finisky.github.io/lelocle-mainspring-suddenly-release/</id>
    <published>2023-07-29T10:00:05.000Z</published>
    <updated>2023-09-07T02:59:39.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;/lelocle-maintain/&quot;&gt;力洛克T41自行洗油保养实录&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;手动上弦的问题并未解决，保养后不久，问题更加严重，仅能上弦约10圈（走时约10小时），便开始不断打滑，同时发出“咔咔”的异响。如小心继续上弦，发条会突然弹松，直接停表。针对此问题搜了许多国内外论坛，都没找到合适的解决方案，此处记录以飨后人。&lt;/p&gt;
&lt;p&gt;前后花了约一个月，买了一根发条，两个发条盒总成件，捏断两根发条内钩，最终购买一套立轮和离合轮（合轮）彻底解决问题。&lt;/p&gt;
&lt;p&gt;先买了一根新发条，换上之后问题依然存在。&lt;/p&gt;
&lt;p&gt;由于出现突然弹松的现象，怀疑发条轴与发条内钩打滑所致，感觉是上链时二者一直打滑，出现“咔咔”的异响。打开看了看发条轴，似乎有一些磨损（它的凸起不太明显，不是非常有经验的老师傅不太容易看出是否磨损），干脆买了一个新的原装发条盒。换上之后问题依旧。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>图说文本生成解码策略</title>
    <link href="https://finisky.github.io/illustrated-decoding-strategies/"/>
    <id>https://finisky.github.io/illustrated-decoding-strategies/</id>
    <published>2023-07-19T01:46:22.000Z</published>
    <updated>2023-07-19T09:59:34.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;之前写过 &lt;a href=&quot;/nucleussampling/&quot;&gt;# Nucleus
Sampling与文本生成中的不同解码策略比较&lt;/a&gt;，不过文中缺乏图例，对于解码过程解释不够清晰，本文作为2.0版加以补充。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;对于文本生成任务，语言模型如何做到对同一个输入生成不同的输出？问题的关键在于解码策略。无论是自编码模型还是自回归模型，都是在解码阶段的每个时间步逐个生成最终文本。所谓解码，就是按照某种策略从候选词表中选择合适的词输出。除了对于模型本身的改进，不同解码策略也对文本生成质量起到重要作用。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>使LLM善假于物: Toolformer</title>
    <link href="https://finisky.github.io/toolformer-summary/"/>
    <id>https://finisky.github.io/toolformer-summary/</id>
    <published>2023-07-16T15:58:16.000Z</published>
    <updated>2023-07-16T15:59:45.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;大模型可以仅凭指令或几个示例就能解决各种新任务，但在一些看似简单的算术或搜索任务上却表现欠佳。俗话说得好，人和动物的区别就是人可以更好地使用工具。于是，Meta
AI提出了&lt;code&gt;Toolformer&lt;/code&gt;，让LLM&lt;code&gt;善假于物&lt;/code&gt;，通过自学使用外部工具。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Toolformer&lt;/code&gt;可以决定调用什么API、何时调用它们、传递什么参数及如何将API返回值融合。&lt;code&gt;Toolformer&lt;/code&gt;以自监督方式训练，每个API仅需要几个示例。它在各种下游任务中显著提升了零样本性能，而不牺牲其核心语言模型能力。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.04761&quot;&gt;# Toolformer: Language
Models Can Teach Themselves to Use Tools&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>数据为王: Textbooks Are All You Need</title>
    <link href="https://finisky.github.io/textbooks-are-all-you-need-summary/"/>
    <id>https://finisky.github.io/textbooks-are-all-you-need-summary/</id>
    <published>2023-07-09T11:48:00.000Z</published>
    <updated>2023-07-09T12:07:12.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;MSR使用“教科书”级的高质量数据训练了仅 1.3B
的面向代码任务的&lt;code&gt;phi-1&lt;/code&gt;模型，在 HumanEval 和 MBPP
上取得了很高的准确率。&lt;/p&gt;
&lt;p&gt;根据模型扩展法则，为提升模型性能，需要从增大算量和模型规模入手。这里则另辟蹊径：从数据质量出发。之前的研究证实：提升数据质量会大幅改变扩展法则趋势，能让小模型达到大模型的效果。本文则在此结论上更进一步，打破了已有的模型扩展法则，证明&lt;strong&gt;高质量的数据甚至可以在使用更少的数据和算量条件下超越大模型的SOTA&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;具体来说，用约 7B token 训练8轮，然后在少于 200M
token的数据上微调得到 1.3B 的模型 &lt;code&gt;phi-1&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.11644&quot;&gt;# Textbooks Are All You
Need&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>训练中文垂类大模型：Lawyer LLaMA</title>
    <link href="https://finisky.github.io/lawyer-llama-summary/"/>
    <id>https://finisky.github.io/lawyer-llama-summary/</id>
    <published>2023-06-19T11:20:16.000Z</published>
    <updated>2023-06-19T17:13:01.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;开源的通用能力大模型越来越多，但真正有用和落地的是在某个领域专精的垂类模型。初看上去，似乎大模型仅需要少量prompt工作就可以很好地在垂类工作，可事实并非如此。不进行领域微调的通用模型可以很快地构建80分的应用，可是大部分的实用场景，需要95甚至98分的模型效果。这也是为什么在各个领域（如金融、车载、虚拟人）大家都在训练或微调自己大模型的原因。&lt;/p&gt;
&lt;p&gt;微调这件事看上去不难，但却有很多未解问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何基于英文大模型继续训练一个中文大模型？&lt;/li&gt;
&lt;li&gt;垂类数据应该在预训练阶段引入，还是指令微调时引入？&lt;/li&gt;
&lt;li&gt;通用指令数据与垂类任务数据的混合比例有什么讲究？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的每个问题都有很多种不同的方案，但限于时间和成本，逐一实验是不可行的，AB测试也会带来额外的成本。所以有趣的事情出现了，各个玩家对自己训练时的细节都讳莫如深，自己训练的时候也都遇到过各种各样奇怪的坑。更有意思的是，即使别人提供了一些细节参考，自己在训练时未必能够复现
:-( 。&lt;/p&gt;
&lt;p&gt;代码库：&lt;a href=&quot;https://github.com/AndrewZhe/lawyer-llama&quot;&gt;# Lawyer
LLaMA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lawyer LLaMA&lt;/code&gt;技术报告：&lt;a
href=&quot;https://arxiv.org/abs/2305.15062&quot;&gt;# Lawyer LLaMA Technical
Report&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
</feed>
