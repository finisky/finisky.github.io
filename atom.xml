<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Finisky Garden</title>
  <icon>https://finisky.github.io/icon.png</icon>
  <subtitle>NLP, 软件工程, 产品设计</subtitle>
  <link href="https://finisky.github.io/atom.xml" rel="self"/>
  
  <link href="https://finisky.github.io/"/>
  <updated>2023-12-14T04:55:11.000Z</updated>
  <id>https://finisky.github.io/</id>
  
  <author>
    <name>finisky</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>小模型的惊人能力: Phi-2</title>
    <link href="https://finisky.github.io/phi2-the-surprising-power-summary/"/>
    <id>https://finisky.github.io/phi2-the-surprising-power-summary/</id>
    <published>2023-12-13T10:18:16.000Z</published>
    <updated>2023-12-14T04:55:11.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;过去半年，MSR发布了一套名为&lt;code&gt;Phi&lt;/code&gt;的小模型（SLMs），取得了卓越的性能表现。其中第一个模型，1.3B
的&lt;code&gt;Phi-1&lt;/code&gt;，实现了在现有SLMs中对Python编码的最佳性能（在HumanEval和MBPP数据集上）。随后，他们将注意力扩展到常识推理和语言理解，并创建了一个新的
1.3B
模型，命名为&lt;code&gt;Phi-1.5&lt;/code&gt;，其性能相当于规模更大5倍的模型。&lt;/p&gt;
&lt;p&gt;最近MSR发布了&lt;code&gt;Phi-2&lt;/code&gt;，一个 2.7B
的语言模型，展示了卓越的推理和语言理解能力，表现出小于 13B
语言模型的最好效果。在各种测试中，&lt;code&gt;Phi-2&lt;/code&gt;与规模大达25倍的模型差不多或获胜，主要归功于模型规模和训练数据方面的创新。MSR已经在Azure
AI Studio模型目录中提供了Phi-2，以促进语言模型的研究和开发。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Phi-2&lt;/code&gt; 未放出细节的技术报告，具体可参考原博客：&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/&quot;&gt;#
Phi-2: The surprising power of small language models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第一代&lt;code&gt;Phi-1&lt;/code&gt;解读：&lt;a
href=&quot;/textbooks-are-all-you-need-summary/&quot;&gt;数据为王: Textbooks Are All
You Need&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Skywork: A More Open Bilingual Foundation Model 简读</title>
    <link href="https://finisky.github.io/skywork-summary/"/>
    <id>https://finisky.github.io/skywork-summary/</id>
    <published>2023-11-10T11:18:16.000Z</published>
    <updated>2023-11-11T02:29:44.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;昆仑的天工模型一直走开源路线，最近放出了技术报告，其中关于预训练模型刷榜作弊的部分引发了广泛的讨论，把大家心照不宣的事情首次放到了台面上
:-)。本文来看下这篇技术报告的亮点(非全文精读，仅摘要有趣的点，细节可阅读原论文)。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.19341&quot;&gt;# Skywork: A More Open
Bilingual Foundation Model&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Windows Explorer Crashes on Right Click Menu Solution</title>
    <link href="https://finisky.github.io/en/explorer-crash-fix/"/>
    <id>https://finisky.github.io/en/explorer-crash-fix/</id>
    <published>2023-10-03T11:20:29.000Z</published>
    <updated>2023-10-03T11:21:29.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;Recently, when I attempt to open the property window of a folder in
File Explorer, it crashes all the time. It bothers me a lot and I
finally fix it by cleaning up the right click context menu.&lt;/p&gt;</summary>
    
    
    
    <category term="Misc" scheme="https://finisky.github.io/categories/Misc/"/>
    
    
  </entry>
  
  <entry>
    <title>十一前洛阳自驾游记</title>
    <link href="https://finisky.github.io/luoyang/"/>
    <id>https://finisky.github.io/luoyang/</id>
    <published>2023-09-28T09:49:43.000Z</published>
    <updated>2023-09-28T09:50:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;趁着十一前的假期，去神都洛阳逛逛。作为一个河南人，河南的景点却基本没怎么去过，这种感觉有点类似住的最近的人常常会迟到一样。&lt;/p&gt;
&lt;p&gt;没有太多规划，原计划是龙门石窟、白马寺和少林寺，不过后来因为一直下雨和略感疲倦，逛完白马寺直接回家。自驾往返共400公里整，大部分是高速，剩下几十公里国道。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
    <category term="自驾游" scheme="https://finisky.github.io/tags/%E8%87%AA%E9%A9%BE%E6%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>LIMA: Less Is More for Alignment 简读</title>
    <link href="https://finisky.github.io/lima-summary/"/>
    <id>https://finisky.github.io/lima-summary/</id>
    <published>2023-09-12T11:18:16.000Z</published>
    <updated>2023-09-12T13:00:25.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;LIMA是五月份的一篇网红文，发表后引起了广泛的讨论。它用极致简约的SFT方案训练了一个不错的模型，希望证实表面对齐假设
(Superficial Alignment Hypothesis)：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;大模型中几乎所有知识都是在预训练中学习的，指令微调只是一个很简单的过程，让模型学到与用户交互的形式。&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;为证明上述假设，作者们推测仅需要少量的指令微调数据(1000条)就可以教会模型产生高质量输出。此外，1000条SFT数据就达到了很好的指令微调效果，也说明了高质量数据对于模型的重要性，这一点与
&lt;a href=&quot;/textbooks-are-all-you-need-summary/&quot;&gt;Textbooks Are All You
Need&lt;/a&gt; 有异曲同工之妙。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.11206&quot;&gt;# LIMA: Less Is More for
Alignment&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>力洛克T41发条突然弹松问题</title>
    <link href="https://finisky.github.io/lelocle-mainspring-suddenly-release/"/>
    <id>https://finisky.github.io/lelocle-mainspring-suddenly-release/</id>
    <published>2023-07-29T10:00:05.000Z</published>
    <updated>2023-09-07T02:59:39.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;/lelocle-maintain/&quot;&gt;力洛克T41自行洗油保养实录&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;手动上弦的问题并未解决，保养后不久，问题更加严重，仅能上弦约10圈（走时约10小时），便开始不断打滑，同时发出“咔咔”的异响。如小心继续上弦，发条会突然弹松，直接停表。针对此问题搜了许多国内外论坛，都没找到合适的解决方案，此处记录以飨后人。&lt;/p&gt;
&lt;p&gt;前后花了约一个月，买了一根发条，两个发条盒总成件，捏断两根发条内钩，最终购买一套立轮和离合轮（合轮）彻底解决问题。&lt;/p&gt;
&lt;p&gt;先买了一根新发条，换上之后问题依然存在。&lt;/p&gt;
&lt;p&gt;由于出现突然弹松的现象，怀疑发条轴与发条内钩打滑所致，感觉是上链时二者一直打滑，出现“咔咔”的异响。打开看了看发条轴，似乎有一些磨损（它的凸起不太明显，不是非常有经验的老师傅不太容易看出是否磨损），干脆买了一个新的原装发条盒。换上之后问题依旧。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>图说文本生成解码策略</title>
    <link href="https://finisky.github.io/illustrated-decoding-strategies/"/>
    <id>https://finisky.github.io/illustrated-decoding-strategies/</id>
    <published>2023-07-19T01:46:22.000Z</published>
    <updated>2023-07-19T09:59:34.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;之前写过 &lt;a href=&quot;/nucleussampling/&quot;&gt;# Nucleus
Sampling与文本生成中的不同解码策略比较&lt;/a&gt;，不过文中缺乏图例，对于解码过程解释不够清晰，本文作为2.0版加以补充。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;对于文本生成任务，语言模型如何做到对同一个输入生成不同的输出？问题的关键在于解码策略。无论是自编码模型还是自回归模型，都是在解码阶段的每个时间步逐个生成最终文本。所谓解码，就是按照某种策略从候选词表中选择合适的词输出。除了对于模型本身的改进，不同解码策略也对文本生成质量起到重要作用。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>使LLM善假于物: Toolformer</title>
    <link href="https://finisky.github.io/toolformer-summary/"/>
    <id>https://finisky.github.io/toolformer-summary/</id>
    <published>2023-07-16T15:58:16.000Z</published>
    <updated>2023-07-16T15:59:45.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;大模型可以仅凭指令或几个示例就能解决各种新任务，但在一些看似简单的算术或搜索任务上却表现欠佳。俗话说得好，人和动物的区别就是人可以更好地使用工具。于是，Meta
AI提出了&lt;code&gt;Toolformer&lt;/code&gt;，让LLM&lt;code&gt;善假于物&lt;/code&gt;，通过自学使用外部工具。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Toolformer&lt;/code&gt;可以决定调用什么API、何时调用它们、传递什么参数及如何将API返回值融合。&lt;code&gt;Toolformer&lt;/code&gt;以自监督方式训练，每个API仅需要几个示例。它在各种下游任务中显著提升了零样本性能，而不牺牲其核心语言模型能力。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.04761&quot;&gt;# Toolformer: Language
Models Can Teach Themselves to Use Tools&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>数据为王: Textbooks Are All You Need</title>
    <link href="https://finisky.github.io/textbooks-are-all-you-need-summary/"/>
    <id>https://finisky.github.io/textbooks-are-all-you-need-summary/</id>
    <published>2023-07-09T11:48:00.000Z</published>
    <updated>2023-07-09T12:07:12.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;MSR使用“教科书”级的高质量数据训练了仅 1.3B
的面向代码任务的&lt;code&gt;phi-1&lt;/code&gt;模型，在 HumanEval 和 MBPP
上取得了很高的准确率。&lt;/p&gt;
&lt;p&gt;根据模型扩展法则，为提升模型性能，需要从增大算量和模型规模入手。这里则另辟蹊径：从数据质量出发。之前的研究证实：提升数据质量会大幅改变扩展法则趋势，能让小模型达到大模型的效果。本文则在此结论上更进一步，打破了已有的模型扩展法则，证明&lt;strong&gt;高质量的数据甚至可以在使用更少的数据和算量条件下超越大模型的SOTA&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;具体来说，用约 7B token 训练8轮，然后在少于 200M
token的数据上微调得到 1.3B 的模型 &lt;code&gt;phi-1&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.11644&quot;&gt;# Textbooks Are All You
Need&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>训练中文垂类大模型：Lawyer LLaMA</title>
    <link href="https://finisky.github.io/lawyer-llama-summary/"/>
    <id>https://finisky.github.io/lawyer-llama-summary/</id>
    <published>2023-06-19T11:20:16.000Z</published>
    <updated>2023-06-19T17:13:01.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;开源的通用能力大模型越来越多，但真正有用和落地的是在某个领域专精的垂类模型。初看上去，似乎大模型仅需要少量prompt工作就可以很好地在垂类工作，可事实并非如此。不进行领域微调的通用模型可以很快地构建80分的应用，可是大部分的实用场景，需要95甚至98分的模型效果。这也是为什么在各个领域（如金融、车载、虚拟人）大家都在训练或微调自己大模型的原因。&lt;/p&gt;
&lt;p&gt;微调这件事看上去不难，但却有很多未解问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何基于英文大模型继续训练一个中文大模型？&lt;/li&gt;
&lt;li&gt;垂类数据应该在预训练阶段引入，还是指令微调时引入？&lt;/li&gt;
&lt;li&gt;通用指令数据与垂类任务数据的混合比例有什么讲究？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的每个问题都有很多种不同的方案，但限于时间和成本，逐一实验是不可行的，AB测试也会带来额外的成本。所以有趣的事情出现了，各个玩家对自己训练时的细节都讳莫如深，自己训练的时候也都遇到过各种各样奇怪的坑。更有意思的是，即使别人提供了一些细节参考，自己在训练时未必能够复现
:-( 。&lt;/p&gt;
&lt;p&gt;代码库：&lt;a href=&quot;https://github.com/AndrewZhe/lawyer-llama&quot;&gt;# Lawyer
LLaMA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lawyer LLaMA&lt;/code&gt;技术报告：&lt;a
href=&quot;https://arxiv.org/abs/2305.15062&quot;&gt;# Lawyer LLaMA Technical
Report&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>提升大模型数学推理能力: 过程监督</title>
    <link href="https://finisky.github.io/process-supervision-reward-model/"/>
    <id>https://finisky.github.io/process-supervision-reward-model/</id>
    <published>2023-06-06T02:18:23.000Z</published>
    <updated>2023-06-06T07:43:30.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;思维链采用逐步推理的方式得到最终结果，如果模型在某一步出现幻想
(Hallucination)，则差之毫厘，谬以千里，后面的错误会被放大，导致错误结果。&lt;/p&gt;
&lt;p&gt;OpenAI最近提出使用过程监督 (Process Supervision)
减少大模型幻想并提升大模型的数学推理能力，所以什么是过程监督？&lt;/p&gt;
&lt;p&gt;过程监督 (Process Supervision) 是相对于之前的结果监督 (Outcome
Supervison)
而言。众所周知，大模型基于人工反馈的强化学习部分需要用到奖励模型 (Reward
Model, RM)，数学推理能力是基于思维链 (Chain of
Thought)。传统的奖励模型采用的是结果监督的方式，仅使用思维链的最终结果进行判别与反馈，而过程监督则是对思维链的每步推理都进行反馈。因此，过程监督是针对思维链和奖励模型的一种改进方案。&lt;/p&gt;
&lt;p&gt;OpenAI开源了过程监督的数据集
PRM800K。论文的核心思想很直观，主要关注在实验设计。&lt;/p&gt;
&lt;p&gt;OpenAI 博客：&lt;a
href=&quot;https://openai.com/research/improving-mathematical-reasoning-with-process-supervision&quot;&gt;#
Improving mathematical reasoning with process supervision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/2305.20050&quot;&gt;# Let&#39;s Verify
Step by Step&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>升级gcc解决编译llama-cpp-python错误</title>
    <link href="https://finisky.github.io/build-llama-cpp-python-error-solution/"/>
    <id>https://finisky.github.io/build-llama-cpp-python-error-solution/</id>
    <published>2023-05-11T09:18:39.000Z</published>
    <updated>2023-05-11T09:28:57.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;安装 &lt;a
href=&quot;https://github.com/oobabooga/text-generation-webui&quot;&gt;text-generation-webui&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&quot;line-numbers language-bash&quot; data-language=&quot;bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;~/text-generation-webui$ pip &lt;span class=&quot;token function&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;token parameter variable&quot;&gt;-r&lt;/span&gt; requirements.txt&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;遇到错误：&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>ERROR: Could not find a version that satisfies the requirement Solution</title>
    <link href="https://finisky.github.io/en/could-not-find-a-version-that-satisfies-the-requirement-solution/"/>
    <id>https://finisky.github.io/en/could-not-find-a-version-that-satisfies-the-requirement-solution/</id>
    <published>2023-05-10T09:05:16.000Z</published>
    <updated>2023-05-10T09:13:16.000Z</updated>
    
    
    <summary type="html">&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code class=&quot;language-none&quot;&gt;$ pip install torch&amp;#x3D;&amp;#x3D;1.12.0
Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement torch&amp;#x3D;&amp;#x3D;1.12.0 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2)
ERROR: No matching distribution found for torch&amp;#x3D;&amp;#x3D;1.12.0&lt;span aria-hidden=&quot;true&quot; class=&quot;line-numbers-rows&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The root cause is that python version is too low (&lt;code&gt;3.6&lt;/code&gt;).
We need to upgrade python to a new version.&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://finisky.github.io/categories/Linux/"/>
    
    
    <category term="python" scheme="https://finisky.github.io/tags/python/"/>
    
    <category term="Linux" scheme="https://finisky.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>贵州4月下旬自驾游记</title>
    <link href="https://finisky.github.io/guizhou/"/>
    <id>https://finisky.github.io/guizhou/</id>
    <published>2023-05-04T01:12:01.000Z</published>
    <updated>2023-05-10T09:13:16.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;好久没出去看大好河山，五一前休假去贵州自驾游，云贵川算是都打了卡。按照惯例，还是提前简单做下攻略，本想将行程设置宽松些，后来发现贵州的大部分景点门票预订（进景区时间）都要精确到小时，才不得不把行程提前细化。&lt;/p&gt;
&lt;p&gt;贵州的主要景点也比较分散，不过都是以省会贵阳为中心放射状排列，本次自驾主要以黔东为主。之前找到一张不错的贵州景点分布图，记不清出处了：&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
    <category term="自驾游" scheme="https://finisky.github.io/tags/%E8%87%AA%E9%A9%BE%E6%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>围炉对谈：OpenAI创始人对GPT-4和ChatGPT的理解</title>
    <link href="https://finisky.github.io/fireside-talk-openai-ceo-summary/"/>
    <id>https://finisky.github.io/fireside-talk-openai-ceo-summary/</id>
    <published>2023-03-30T11:02:03.000Z</published>
    <updated>2023-03-30T11:05:14.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;3月22日，NVIDIA的CEO黄仁勋与OpenAI的创始人Ilya
Sutskever进行了围炉对谈，通过视频可以更好地了解OpenAI是如何走到今天，又是如何理解ChatGPT和GPT-4这些大模型的。不过毕竟是非正式访谈，思路和观点略有发散，本文提取访谈中一些有意思的观点供参考。&lt;/p&gt;
&lt;p&gt;BTW，网上的中文完整字幕翻译对某些观点的翻译解读有误，建议看原视频。&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://blogs.nvidia.com/blog/2023/03/22/sutskever-openai-gtc/&quot;&gt;#
AI Opener: OpenAI’s Sutskever in Conversation With Jensen Huang&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>力洛克T41自行洗油保养实录</title>
    <link href="https://finisky.github.io/lelocle-maintain/"/>
    <id>https://finisky.github.io/lelocle-maintain/</id>
    <published>2023-03-05T15:49:05.000Z</published>
    <updated>2023-08-23T04:18:07.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;戴了十几年的力洛克，一年前开始走时不准，最近每天能慢上一分钟，手动上弦似乎也有些问题，总是上不满弦，怀疑与之前疫情在家总手动上弦有关系
(最初怀疑发条断了)。&lt;/p&gt;
&lt;p&gt;距上次保养已经5年有余，天梭官方授权的店保养一次 (所谓完全服务)
约一千块，而买块新的ETA-2824-2机芯也就差不多这个价，所以再去保养显得非常不划算。老爷子年轻时玩表修表，有此家学，再加上网上有许多机芯拆解洗油点油视频，看起来也不甚困难，跃跃欲试，决定自行保养维护。&lt;/p&gt;
&lt;p&gt;前后历时一个月才保养完毕，趟坑无数。现在看来，动手时显然低估了保养洗油的难度，加之中间遇到的诸多难题，本想从玩表的过程获取些操作的成就感，不想却收获了诸多挫败感。修完后才感叹，授权店收一千块算是良心价了
:-) 。好在最终问题完美解决，记录下保养过程。&lt;/p&gt;</summary>
    
    
    
    <category term="Life" scheme="https://finisky.github.io/categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>Chain-of-Thought Prompting 简读</title>
    <link href="https://finisky.github.io/chain-of-thought-prompting-summary/"/>
    <id>https://finisky.github.io/chain-of-thought-prompting-summary/</id>
    <published>2023-03-01T11:39:00.000Z</published>
    <updated>2023-03-01T11:45:49.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;语言模型越来越大，但更大的模型并没有显示出更强的计算和推理能力。去年Google提出了Chain-of-Thought
(CoT)
的方案，通过chain-of-thought提示，让模型逐步推断，使大模型的推理能力显著提升。本文来看一下chain-of-thought的原理。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;Chain-of-Thought Prompting
Elicits Reasoning in Large Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;&gt;Language
Models Perform Reasoning via Chain of Thought&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>大模型训练不稳定问题及解决方案</title>
    <link href="https://finisky.github.io/llm-training-instability-solution/"/>
    <id>https://finisky.github.io/llm-training-instability-solution/</id>
    <published>2023-02-15T02:08:12.000Z</published>
    <updated>2023-02-15T09:47:17.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;大规模语言模型的春风已经吹遍大地，大家都惊叹于大模型出色的对话能力，但是在训练大模型时遇到的训练不稳定问题(&lt;strong&gt;training
instabilities&lt;/strong&gt;)，可能关注的人并不太多。所谓量变引起质变，模型每大一个量级，就可能会出现一些意想不到的问题，比如莫名其妙的训练崩溃。当然，也有好的方面，在模型有一定规模后，是否有可能表现出一些弱智能，也很难说。&lt;/p&gt;
&lt;p&gt;言归正传，今天聊聊在训练10B以上模型时遇到的训练不稳定现象，问题原因及当前的解法。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="NLG" scheme="https://finisky.github.io/tags/NLG/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Google拟发布ChatGPT的竞争对手Bard</title>
    <link href="https://finisky.github.io/google-open-bard-to-trusted-testers/"/>
    <id>https://finisky.github.io/google-open-bard-to-trusted-testers/</id>
    <published>2023-02-08T11:55:39.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;ChatGPT的大火让Google也坐不住了，许多人认为这一波Google已落后一个身位。坊间甚至传言创始人谢尔盖・布林都已“躬身入局”，亲自写代码了。上面的说法可以当八卦看来一乐，不过昨天微软官宣Bing和Edge浏览器要集成ChatGPT时，Google也不甘示弱，表示也要上线大模型&lt;code&gt;Bard&lt;/code&gt;
(这个名字倒也颇具浪漫主义气质：吟游诗人)。&lt;/p&gt;</summary>
    
    
    
    <category term="News" scheme="https://finisky.github.io/categories/News/"/>
    
    
  </entry>
  
  <entry>
    <title>大模型分布式训练的并行策略</title>
    <link href="https://finisky.github.io/how-to-train-large-language-model/"/>
    <id>https://finisky.github.io/how-to-train-large-language-model/</id>
    <published>2023-02-02T15:32:10.000Z</published>
    <updated>2023-02-08T11:57:54.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;随着神经网络模型规模的不断增大，对硬件的显存和算力提出了新的要求。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如&lt;code&gt;Megatron-Turing NLG&lt;/code&gt;有
530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://coriva.eu.org/images/nlp/trendofnlpmodelsize.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;同时，模型是一个有机的整体，简单增加机器数量并不能提升算力，需要有并行策略和通信设计，才能实现高效的并行训练。本文简要介绍目前主流的几种并行策略：数据并行，张量并行，流水线并行和混合并行。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.11990&quot;&gt;# Using DeepSpeed and
Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative
Language Model&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="https://finisky.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="https://finisky.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://finisky.github.io/tags/Transformer/"/>
    
    <category term="Language Model" scheme="https://finisky.github.io/tags/Language-Model/"/>
    
  </entry>
  
</feed>
