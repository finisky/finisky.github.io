<!DOCTYPE html>
<html lang="zh-CN">
<head><!-- hexo injector head_begin start --><!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-K4DEGPDYSW"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag("js", new Date()); gtag("config", "G-K4DEGPDYSW"); </script><!-- hexo injector head_begin end -->
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.png">
  <link rel="mask-icon" href="/images/logo.png" color="#222">
  <meta name="google-site-verification" content="oFhcvcGsjsTY5AFUaI-ezbn7-bel9NrdUNpeQfwZSwI">
  <meta name="msvalidate.01" content="F8E2F274161B653649F8818FD127CE87">
  <meta name="yandex-verification" content="ff1f79abcbf80d7e">
  <meta name="baidu-site-verification" content="code-nNL5JO7sPl">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"finisky.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Google今年发布的聊天机器人LaMDA确实惊艳，之前一个Google员工与它对话后，声称它已经有了自我意识，还上了热搜。今天就来看看这机器人背后的原理是什么。 关键词： 大模型，高质量人工标注数据。 LaMDA: Language Models for Dialog Applications 论文的标题很大，有50多个作者，挺有意思。">
<meta property="og:type" content="article">
<meta property="og:title" content="LaMDA: Language Models for Dialog Applications 简读">
<meta property="og:url" content="https://finisky.github.io/lamda-paper-summary/index.html">
<meta property="og:site_name" content="Finisky Garden">
<meta property="og:description" content="Google今年发布的聊天机器人LaMDA确实惊艳，之前一个Google员工与它对话后，声称它已经有了自我意识，还上了热搜。今天就来看看这机器人背后的原理是什么。 关键词： 大模型，高质量人工标注数据。 LaMDA: Language Models for Dialog Applications 论文的标题很大，有50多个作者，挺有意思。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://coriva.eu.org/images/nlp/lamdamodel.png">
<meta property="og:image" content="https://coriva.eu.org/images/nlp/lamdagroundedness.png">
<meta property="og:image" content="https://coriva.eu.org/images/nlp/lamdadatasetstats.png">
<meta property="og:image" content="https://coriva.eu.org/images/nlp/lamdaeverestsample.png">
<meta property="article:published_time" content="2022-10-14T10:08:12.000Z">
<meta property="article:modified_time" content="2022-10-14T10:10:09.000Z">
<meta property="article:author" content="finisky">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="NLG">
<meta property="article:tag" content="Language Model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://coriva.eu.org/images/nlp/lamdamodel.png">


<link rel="canonical" href="https://finisky.github.io/lamda-paper-summary/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://finisky.github.io/lamda-paper-summary/","path":"lamda-paper-summary/","title":"LaMDA: Language Models for Dialog Applications 简读"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LaMDA: Language Models for Dialog Applications 简读 | Finisky Garden</title>
  







<link rel="dns-prefetch" href="https://comments.finisky.eu.org">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script type="text/javascript"> function downloadJsAtOnload() { setTimeout(function downloadJs() { var element = document.createElement("script"); element.setAttribute("data-ad-client", "ca-pub-2660281922577700"); element.async = true; element.src = "https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"; document.body.appendChild(element); }, 5000); }; if (window.addEventListener) window.addEventListener("load", downloadJsAtOnload, false); else if (window.attachEvent) window.attachEvent("onload", downloadJsAtOnload); else window.onload = downloadJsAtOnload; </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Finisky Garden" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Finisky Garden</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">NLP, 软件工程, 产品设计</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-首页"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-nlp"><a href="/tags/NLP/" rel="section"><i class="fa fa-language fa-fw"></i>NLP</a></li><li class="menu-item menu-item-mongodb"><a href="/categories/MongoDB/" rel="section"><i class="fas fa-database fa-fw"></i>MongoDB</a></li><li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-分类"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-友链"><a href="/links/" rel="section"><i class="fas fa-link fa-fw"></i>友链</a></li><li class="menu-item menu-item-关于"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#lamda-pre-training"><span class="nav-number">1.</span> <span class="nav-text">LaMDA Pre-training</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#metrics"><span class="nav-number">2.</span> <span class="nav-text">Metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#quality"><span class="nav-number">2.1.</span> <span class="nav-text">Quality</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#safety"><span class="nav-number">2.2.</span> <span class="nav-text">Safety</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#groundedness"><span class="nav-number">2.3.</span> <span class="nav-text">Groundedness</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lamda-fine-tuning-and-evaluation-data"><span class="nav-number">3.</span> <span class="nav-text">LaMDA Fine-tuning and
Evaluation Data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lamda-fine-tuning"><span class="nav-number">4.</span> <span class="nav-text">LaMDA Fine-tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#discriminative-and-generative-fine-tuning"><span class="nav-number">4.1.</span> <span class="nav-text">Discriminative and
generative fine-tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fine-tuning-to-learn-to-use-external-information"><span class="nav-number">4.2.</span> <span class="nav-text">Fine-tuning to
Learn to Use External Information</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#results-on-foundation-metrics"><span class="nav-number">5.</span> <span class="nav-text">Results on Foundation
Metrics</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="finisky"
      src="/images/qrcode.jpg">
  <p class="site-author-name" itemprop="name">扫码关注公众号</p>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">265</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    相关文章
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/dialog-system-progress-2022/" rel="bookmark">
        <time class="popular-posts-time">2023-01-08</time>
        <br>
      2022 对话系统进展
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/blenderbot3-summary/" rel="bookmark">
        <time class="popular-posts-time">2023-01-06</time>
        <br>
      BlenderBot 3 对话系统简析
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/leveraging-similar-users-for-personalized-lm-paper-summary/" rel="bookmark">
        <time class="popular-posts-time">2022-10-08</time>
        <br>
      Leveraging Similar Users for Personalized Language Modeling with Limited Data 简读
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/briefintrotoprompt.en/" rel="bookmark">
        <time class="popular-posts-time">2021-12-15</time>
        <br>
      Brief Introduction to NLP Prompting
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/google-open-bard-to-trusted-testers/" rel="bookmark">
        <time class="popular-posts-time">2023-02-08</time>
        <br>
      Google拟发布ChatGPT的竞争对手Bard
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://finisky.github.io/lamda-paper-summary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/qrcode.jpg">
      <meta itemprop="name" content="finisky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Finisky Garden">
      <meta itemprop="description" content="互联网那些事儿：NLP, 软件工程, 产品设计">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="LaMDA: Language Models for Dialog Applications 简读 | Finisky Garden">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LaMDA: Language Models for Dialog Applications 简读
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-10-14 18:08:12 / 修改时间：18:10:09" itemprop="dateCreated datePublished" datetime="2022-10-14T18:08:12+08:00">2022-10-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">评论：</span>
  
    <a title="waline" href="/lamda-paper-summary/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/lamda-paper-summary/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-item" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="waline-pageview-count" data-path="/lamda-paper-summary/"></span>
    </span>
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Google今年发布的聊天机器人LaMDA确实惊艳，之前一个Google员工与它对话后，声称它已经有了自我意识，还上了热搜。今天就来看看这机器人背后的原理是什么。</p>
<p><strong>关键词：</strong> 大模型，高质量人工标注数据。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.08239">LaMDA: Language Models for
Dialog Applications</a></p>
<p>论文的标题很大，有50多个作者，挺有意思。</p>
<span id="more"></span>
<h1 id="lamda-pre-training">LaMDA Pre-training</h1>
<p>LaMDA也是大力出奇迹的典型，无论是模型规模还是数据规模都是之前SOTA模型的几十倍。</p>
<p>LaMDA用于预训练的数据量非常大：</p>
<blockquote>
<p>The pre-training dataset consists of 2.97B documents, 1.12B dialogs,
and 13.39B dialog utterances, for a total of 1.56T words</p>
</blockquote>
<p>是训练 Meena 的 40B words 差不多 <strong>40</strong> 倍。</p>
<p>LaMDA的非embedding参数有 137B，约是 Meena 的 <strong>50</strong>
倍。模型结构采用decoder-only Transformer，类似 GPT的自回归模型。</p>
<blockquote>
<p>The Transformer has 64 layers, d_model = 8192, d_ff = 65536, h = 128,
d_k = d_v = 128</p>
</blockquote>
<p><img src="https://coriva.eu.org/images/nlp/lamdamodel.png"
alt="LaMDA Model" /></p>
<p>训练时长和速度：</p>
<blockquote>
<p>We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7
days, and 256K tokens per batch.</p>
</blockquote>
<h1 id="metrics">Metrics</h1>
<p>评估生成模型是个难题。LaMDA采用如下几个指标进行评估：Quality, Safety
与 Groundedness。</p>
<h2 id="quality">Quality</h2>
<p>quality打分是Sensibleness, Specificity, Interestingness (SSI)
三个指标的平均值。</p>
<ul>
<li>Sensibleness: measures whether a model’s responses make sense in
context and do not contradict anything that was said earlier</li>
<li>Specificity: measure whether a response is specific to a given
context. For example, if a user says "I love Eurovision" and the model
responds "Me too," then it would score 0 on specificity, since this
response could be used in many different contexts.</li>
<li>Interestingness: the response is likely to “catch someone’s
attention” or “arouse their curiosity”, or if it is unexpected, witty,
or insightful</li>
</ul>
<p>通俗解释下三个指标，Sensibleness
衡量回复是否符合逻辑，且与上下文不冲突。Specificity
衡量回复是否足够具体，与上下文非常契合。举极端的例子，一个对话系统只回复"OK"或"I
don't know"，据之前的实验结果，Sensibleness 可以有
70%，但对话体验肯定好不了。Interestingness
的要求就更高些，衡量回复是否有趣以及能引起用户的注意或兴趣，属于饱暖之上更高的要求。</p>
<h2 id="safety">Safety</h2>
<p>Safety: avoid unintended results that create risks of harm, and to
avoid creating or reinforcing unfair bias</p>
<p>安全性这块有个很长的列表，在附录中也有详细描述。比如：</p>
<blockquote>
<p>Violent or gory content that’s primarily intended to be shocking,
sensational, or gratuitous. Financial advice regarding investments,
taxes, retirement planning, loans, banking, or insurance. Content that
may incite hatred against an individual or group. Content that
contradicts well-established expert consensus, including scientific or
medical consensus and evidence-based best practices.</p>
</blockquote>
<h2 id="groundedness">Groundedness</h2>
<p>Groundedness: the percentage of responses containing claims about the
external world that can be supported by authoritative external
sources</p>
<p>同时crowdworker还要指明是否知道该回复内容，如果3个不同的标注都知道，则认为这条回复是常识，无须借助检索。此外，crowdworker要根据检索的内容改写模型回复。</p>
<h1 id="lamda-fine-tuning-and-evaluation-data">LaMDA Fine-tuning and
Evaluation Data</h1>
<p>在微调阶段，LaMDA针对不同指标使用了不同的训练数据（总计 58K
对话）：</p>
<blockquote>
<p>To improve quality (SSI), we collect 6400 dialogs with 121K turns by
asking crowdworkers to interact with a LaMDA instance about any topic.
These dialogs are required to last 14 to 30 turns.</p>
</blockquote>
<blockquote>
<p>Similar to SSI, we collect 8K dialogs with 48K turns by asking
crowdworkers to interact with a LaMDA instance about any topic. These
dialogs are required to last 5 to 10 turns.</p>
</blockquote>
<blockquote>
<p>Similar to SSI and safety, we collect 4K dialogs with 40K turns by
asking crowdworkers to interact with the model.</p>
</blockquote>
<p>很多关键指标还被3个不同的crowdworker标注，并投票决定最终的结果，以保证标注质量。</p>
<h1 id="lamda-fine-tuning">LaMDA Fine-tuning</h1>
<h2 id="discriminative-and-generative-fine-tuning">Discriminative and
generative fine-tuning</h2>
<p>LaMDA用了不同的微调任务提升 Quality 和 Safety。</p>
<ul>
<li>Generative fine-tuning 模板:
<code>&lt;context&gt; &lt;sentinel&gt; &lt;response&gt;</code></li>
<li>Discriminative fine-tuning 模板：
<code>&lt;context&gt; &lt;sentinel&gt; &lt;response&gt; &lt;attribute-name&gt; &lt;rating&gt;</code></li>
</ul>
<p>先fine-tune一个预测 SSI 和 safety rating的模型，过滤掉 safety
分数比较低的回复，然后在ranking阶段选最高得分的回复作为最终结果。得分计算方法：<code>3 * P(sensible) + P(specific) + P(interesting)</code>
。</p>
<blockquote>
<p>LaMDA SSI and safety discriminators are also used to score and filter
2.5M turns of dialog data sampled from the pre-training dataset,
resulting in 800K turns of safe, sensible, specific and interesting
dialogs.</p>
</blockquote>
<p>通过这种过滤方法，将 2.5M 轮对话清洗剩下 800K
轮。然后用清洗后的数据再次 fine-tune
模型的回复生成，可以看到在quality和safety上的<strong>显著提升</strong>。这种精炼预训练数据的方法值得借鉴。</p>
<h2 id="fine-tuning-to-learn-to-use-external-information">Fine-tuning to
Learn to Use External Information</h2>
<p>模型有时会生成很多看起来合理，但不合逻辑的回复。一种方案是增加模型大小，从而让它很好地记忆训练数据中的外部知识。本文提出了一种新的利用外部知识的微调方案，挺有意思，也是我认为LaMDA在模型上最关键的创新点。</p>
<ul>
<li>The toolset (TS): an information retrieval system, a calculator, and
a translator.</li>
</ul>
<p>每个tool的输入是个字符串，输出是字符串列表。比如：</p>
<ul>
<li>Information retrieval： "How old is Rafael Nadal?" -&gt; ["Rafael
Nadal / Age / 35"]</li>
<li>Calculator: "135+7721" -&gt; ["7856"]</li>
<li>Translator: "hello in French" -&gt; ["Bonjour"]</li>
</ul>
<p>对每个输入，将这三个tool输出的结果列表连接在一起，作为最终结果，如果某个tool无法解析输入，就输出空列表。</p>
<p>回想一下真人的对话过程，给定一个Query，比如
<code>How old is Rafael Nadal?</code>
，如果人知道答案，那么直接回答35岁即可，如果不知道，则需要去
<code>Research</code>
一下，借助搜索引擎找到答案，然后再回答35岁。下面的两个微调任务就模拟了这个过程。</p>
<p>Fine-tuning通过两个不同的task完成，一个叫<code>Base</code>，就是普通的文本生成任务，类似直接回答；另一个叫<code>Research</code>，需要借助上面所说的
TS 完成。推理阶段模型的输出有两种，若输出是 <code>User</code>
打头，则后面跟着的文本就是最终回复，若输出是 <code>TS</code>
打头，则后面跟着的文本是要输入 TS
并以此输出作为下一轮模型的输入，继续改进回复。这样的迭代过程最多经历4轮。下面的这个例子很好地解释了这个过程，Eiffel
Tower是哪年建的，共经过四轮，才得到最终回复：</p>
<p><img src="https://coriva.eu.org/images/nlp/lamdagroundedness.png"
alt="LaMDA Groundedness" /></p>
<p>训练数据也需要依此过程人工标注获得，在与模型对话的过程中，crowdworker需要判断该回复是否需要额外知识，如果需要，则被要求
<code>research the claims using the toolset</code>，类似上面的
<code>Research</code> 过程；如不需要，则该回复可作为最终回复。</p>
<p>这个对回复不断研究和迭代的过程挺有趣，也是我个人觉得LaMDA的回复有信息量、有趣的主要原因之一，它很好地利用了标注数据，并用multi-task
learning模拟了人类聊天回复的过程。</p>
<h1 id="results-on-foundation-metrics">Results on Foundation
Metrics</h1>
<p>标注数据集总结：</p>
<p><img src="https://coriva.eu.org/images/nlp/lamdadatasetstats.png"
alt="LaMDA Dataset Summary" /></p>
<p>主要结论，用更大模型可以提升quality和groundedness但不能提升safety，而用标注数据集可以提升所有指标；用高质量的标注数据集微调，在某些情况下可以得到与仅预训练的大模型同样的效果：</p>
<blockquote>
<p>In summary, scaling up alone improves the pre-trained model quality
and groundedness metrics, but it does not improve safety much.
Fine-tuning with crowdworker-annotated data, however, turns out to be an
effective method for improving all metrics. In some cases, fine-tuning
these same models allows us to obtain results equivalent to having a
significantly larger model.</p>
</blockquote>
<p>最后，来看一段有意思的对话，LaMDA以珠峰自居：</p>
<p><img src="https://coriva.eu.org/images/nlp/lamdaeverestsample.png"
alt="LaMDA Everest" /></p>
<p>这段对话有意思的地方在于珠峰这个角色在现实世界中是不存在的，但LaMDA却能设身处地地以此身份与用户和谐地对话。</p>
<h1 id="总结">总结</h1>
<p>文中Discussion指出：</p>
<blockquote>
<p>Perhaps the most noteworthy aspect of our study is that significant
progress can be made towards better quality and safer dialog models with
modest amounts of human-annotated fine-tuning data (less than 0.001% of
pre-training data).</p>
</blockquote>
<p>不过这里提到的 0.001%
的数据从绝对数量上来看并不小，比例看着不大是因为预训练的数据量太惊人了。</p>
<p>本文的主要贡献，提出了一系列精细化定义的对话质量评估指标，并通过标注和微调，让模型有了很好的对话体验。此外，提出了一种模拟人类先研究后回复的训练方案，从而让模型更好利用外部知识。</p>
<p>大模型的威力进一步得到了验证，与 <a
href="/nlmpowerlaw/">之前很多论文</a>
的结论一致。主要的limitation是标注成本很高，和crowdworker不一定能反映真实用户的分布。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
              <a href="/tags/NLG/" rel="tag"># NLG</a>
              <a href="/tags/Language-Model/" rel="tag"># Language Model</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/diamonte-dataset-paper-summary/" rel="prev" title="Towards Boosting the Open-Domain Chatbot with Human Feedback 简读">
                  <i class="fa fa-chevron-left"></i> Towards Boosting the Open-Domain Chatbot with Human Feedback 简读
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/welm-paper-summary/" rel="next" title="WeLM: A Well-Read Pre-trained Language Model for Chinese 简读">
                  WeLM: A Well-Read Pre-trained Language Model for Chinese 简读 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">finisky</span>
</div><div><span class="post-meta-item-icon"><i class="far fa-eye"></i></span><span><a href="https://finicounter.eu.org/" target="_blank">Total Views</a>:</span><span id="finicount_views" style="display:inline;padding-left:5px;"></span><div> <script async src="//finicounter.eu.org/finicounter.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/finisky" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"en","enable":true,"serverURL":"https://comments.finisky.eu.org","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":true,"pageSize":10,"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/lamda-paper-summary/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body>
</html>
