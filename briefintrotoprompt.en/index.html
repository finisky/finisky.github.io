<!DOCTYPE html>
<html lang="en">
<head><!-- hexo injector head_begin start --><!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-K4DEGPDYSW"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag("js", new Date()); gtag("config", "G-K4DEGPDYSW"); </script><!-- hexo injector head_begin end -->
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.png">
  <link rel="mask-icon" href="/images/logo.png" color="#222">
  <meta name="google-site-verification" content="oFhcvcGsjsTY5AFUaI-ezbn7-bel9NrdUNpeQfwZSwI">
  <meta name="msvalidate.01" content="F8E2F274161B653649F8818FD127CE87">
  <meta name="yandex-verification" content="f6d91994ba785a63">
  <meta name="baidu-site-verification" content="code-nNL5JO7sPl">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"finisky.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Prompting is one of the hottest NLP techniques. This is a brief introduction to prompting by three questions: what&#39;s prompting, why prompting and how to prompting. As a brief introduction, we do not c">
<meta property="og:type" content="article">
<meta property="og:title" content="Brief Introduction to NLP Prompting">
<meta property="og:url" content="https://finisky.github.io/briefintrotoprompt.en/index.html">
<meta property="og:site_name" content="Finisky Garden">
<meta property="og:description" content="Prompting is one of the hottest NLP techniques. This is a brief introduction to prompting by three questions: what&#39;s prompting, why prompting and how to prompting. As a brief introduction, we do not c">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://coriva.eu.org/images/nlp/prefixtuning.png">
<meta property="og:image" content="https://coriva.eu.org/images/nlp/ptuning.png">
<meta property="og:image" content="https://coriva.eu.org/images/nlp/promptparametercomp.png">
<meta property="article:published_time" content="2021-12-15T10:15:46.000Z">
<meta property="article:modified_time" content="2023-01-24T14:30:12.000Z">
<meta property="article:author" content="finisky">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="NLG">
<meta property="article:tag" content="NLU">
<meta property="article:tag" content="Language Model">
<meta property="article:tag" content="Prompt">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://coriva.eu.org/images/nlp/prefixtuning.png">


<link rel="canonical" href="https://finisky.github.io/briefintrotoprompt.en/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://finisky.github.io/briefintrotoprompt.en/","path":"briefintrotoprompt.en/","title":"Brief Introduction to NLP Prompting"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Brief Introduction to NLP Prompting | Finisky Garden</title>
  







<link rel="dns-prefetch" href="https://comments.finisky.eu.org">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script type="text/javascript"> function downloadJsAtOnload() { setTimeout(function downloadJs() { var element = document.createElement("script"); element.setAttribute("data-ad-client", "ca-pub-2660281922577700"); element.async = true; element.src = "https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"; document.body.appendChild(element); }, 5000); }; if (window.addEventListener) window.addEventListener("load", downloadJsAtOnload, false); else if (window.attachEvent) window.attachEvent("onload", downloadJsAtOnload); else window.onload = downloadJsAtOnload; </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Finisky Garden" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Finisky Garden</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">NLP, 软件工程, 产品设计</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-首页"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-nlp"><a href="/tags/NLP/" rel="section"><i class="fa fa-language fa-fw"></i>NLP</a></li><li class="menu-item menu-item-mongodb"><a href="/categories/MongoDB/" rel="section"><i class="fas fa-database fa-fw"></i>MongoDB</a></li><li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-分类"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-友链"><a href="/links/" rel="section"><i class="fas fa-link fa-fw"></i>友链</a></li><li class="menu-item menu-item-关于"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#whats-prompting"><span class="nav-number">1.</span> <span class="nav-text">What&#39;s Prompting</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#why-prompting"><span class="nav-number">2.</span> <span class="nav-text">Why Prompting</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#how-to-prompting"><span class="nav-number">3.</span> <span class="nav-text">How to Prompting</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#discrete-prompt-templates"><span class="nav-number">3.1.</span> <span class="nav-text">Discrete Prompt Templates</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pet"><span class="nav-number">3.1.1.</span> <span class="nav-text">PET</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#autoprompt"><span class="nav-number">3.1.2.</span> <span class="nav-text">AutoPrompt</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#continuous-prompt-templates"><span class="nav-number">3.2.</span> <span class="nav-text">Continuous Prompt Templates</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#prefix-tuning"><span class="nav-number">3.2.1.</span> <span class="nav-text">Prefix-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#p-tuning"><span class="nav-number">3.2.2.</span> <span class="nav-text">P-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prompt-tuning"><span class="nav-number">3.2.3.</span> <span class="nav-text">Prompt Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#how-to-init-prompt"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">How to Init Prompt?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#how-to-choose-prompt-length"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">How to Choose Prompt Length?</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#summary"><span class="nav-number">4.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="finisky"
      src="/images/qrcode.jpg">
  <p class="site-author-name" itemprop="name">扫码关注公众号</p>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">217</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    Related Posts
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/briefintrotoprompt/" rel="bookmark">
        <time class="popular-posts-time">2021-12-14</time>
        <br>
      NLP Prompt技术简介
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/lora/" rel="bookmark">
        <time class="popular-posts-time">2022-05-13</time>
        <br>
      LoRA: Low-Rank Adaptation of Large Language Models 简读
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/chain-of-thought-prompting-summary/" rel="bookmark">
        <time class="popular-posts-time">2023-03-01</time>
        <br>
      Chain-of-Thought Prompting 简读
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/nlpadapters/" rel="bookmark">
        <time class="popular-posts-time">2022-05-01</time>
        <br>
      NLP中的Adapters是什么？
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/nlmpowerlaw/" rel="bookmark">
        <time class="popular-posts-time">2022-01-05</time>
        <br>
      Scaling Laws for Neural Language Models简读
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://finisky.github.io/briefintrotoprompt.en/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/qrcode.jpg">
      <meta itemprop="name" content="finisky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Finisky Garden">
      <meta itemprop="description" content="互联网那些事儿：NLP, 软件工程, 产品设计">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Brief Introduction to NLP Prompting | Finisky Garden">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Brief Introduction to NLP Prompting
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-15 18:15:46" itemprop="dateCreated datePublished" datetime="2021-12-15T18:15:46+08:00">2021-12-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-01-24 22:30:12" itemprop="dateModified" datetime="2023-01-24T22:30:12+08:00">2023-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">评论: </span>
  
    <a title="waline" href="/briefintrotoprompt.en/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/briefintrotoprompt.en/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-item" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="waline-pageview-count" data-path="/briefintrotoprompt.en/"></span>
    </span>
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Prompting is one of the hottest NLP techniques. This is a brief
introduction to prompting by three questions: what's prompting, why
prompting and how to prompting. As a brief introduction, we do not cover
too much details but try to summarize the main idea of prompting. For
more details, please refer to the original papers.</p>
<h1 id="whats-prompting">What's Prompting</h1>
<p>I don't find a rigorous defintion for prompting. Just quoting some
pieces from papers.</p>
<blockquote>
<p>Users prepend a natural language task instruction and a few examples
to the task input; then generate the output from the LM. This approach
is known as in-context learning or prompting.</p>
<p>By: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00190"># Prefix-Tuning:
Optimizing Continuous Prompts for Generation</a></p>
</blockquote>
<p>This description brought two concepts:
<code>in-context learning</code> and <code>prompting</code>.</p>
<p>Another explantion from probability perspective:</p>
<blockquote>
<p>Prompting is the approach of adding extra information for the model
to condition on during its generation of Y .</p>
<p>By: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.08691"># The Power of Scale
for Parameter-Efficient Prompt Tuning</a></p>
</blockquote>
<span id="more"></span>
<p>For example, to analyze the sentiment of sentence
<code>Best pizza ever!</code>, we construct a template like this:</p>
<pre class="line-numbers language-none"><code class="language-none">Best pizza ever! It was ___.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>Then the sentiment analysis task is reduced to a cloze task. The
prediction probability of <code>great</code> would be much higher than
<code>bad</code>. Therefore, through constructing proper templates, we
are able to extract the potential of pretrained language models.</p>
<p>Pattern-Exploiting Training <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.07676">PET</a> explains why prompting
works:</p>
<blockquote>
<p>This illustrates that solving a task from only a few examples becomes
much easier when we also have a task description, i.e., a textual
explanation that helps us understand what the task is about.</p>
</blockquote>
<p>The main difference between pretrain-finetuning and prompt-tuning is
that the former makes the model fits the downstream task, while the
latter elicits the knowledge from the model by prompting.</p>
<h1 id="why-prompting">Why Prompting</h1>
<p>Recently language models are typically of large volumes of
parameters. Given kinds of downstream tasks, fintuning produces a model
copy for each of them. A straightforward approach is light-weight
finetuning, aka freeze most of the parameters while only a small number
of parameters are finetuned.</p>
<p>On the extreme end, GPT-3 can be deployed without any task-specific
tuning. By prompting, we can easily accomplish few-shot learning or even
zero-shot learning.</p>
<p>Moreover, by designing good templates, the model can achieve
comparable or even better performance on many benchmarks.</p>
<p>Besides, for the sake of negligible modification to the model,
enabling personalization is more viable. Because we can only store small
number of parameters for each user to accomplish user-specific
customizations.</p>
<h1 id="how-to-prompting">How to Prompting</h1>
<p>The next question, how to find a proper prompt template?</p>
<p>In this section, we list the main idea of the papers with different
prompt templates. For simplicity, we keep the same notation as the
original paper.</p>
<h2 id="discrete-prompt-templates">Discrete Prompt Templates</h2>
<p>Design prompts is non-trivial and a little bit tricky. Good
understanding of language model and tasks are required. Many efforts
should be paid to find suitable pattern which leads to better
performance.</p>
<p>Such discrete prompt is named <code>hard prompt</code>.</p>
<h3 id="pet">PET</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.07676"># Exploiting Cloze
Questions for Few Shot Text Classification and Natural Language
Inference</a></p>
<p>PET reformulates the text input as cloze-style phrases to help
language models understand a task. PET proposed different templates for
different tasks. The templates look like this (<code>a</code> and
<code>b</code> are text inputs):</p>
<pre class="line-numbers language-none"><code class="language-none">It was ___. a
a. All in all, it was ___.
a ( ___ ) b
[ Category: ___ ] a b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="autoprompt">AutoPrompt</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.15980"># AutoPrompt: Eliciting
Knowledge from Language Models with Automatically Generated
Prompts</a></p>
<p>Essentially, AutoPrompt is a sort of discrete natural language
template. The template has three parts: sentence, trigger tokens and
prediction token.</p>
<pre class="line-numbers language-none"><code class="language-none">&#123;sentence&#125; [T][T] . . [T] [P].<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>AutoPrompt generates trigger tokens by maximizing the label liklihood
over the labeled examples. These trigger tokens are regarded as the
related knowledge of the task. Based on this generated template, the
model generalizes to each specific task.</p>
<h2 id="continuous-prompt-templates">Continuous Prompt Templates</h2>
<p>Generating natural language templates requires manual efforts and
guesswork. Actually, the prompt is not necessarily to be natural
language, it can be of differnet styles such as a continuous vector. As
a result, another line of work try to develop continuous prompt
templates which is obtained via training.</p>
<p>Such continuous prompt is named <code>soft prompt</code>.</p>
<h3 id="prefix-tuning">Prefix-Tuning</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00190"># Prefix-Tuning:
Optimizing Continuous Prompts for Generation</a></p>
<p><img src="https://coriva.eu.org/images/nlp/prefixtuning.png"
alt="Prefix-Tuning" /></p>
<p>Prefix-Tuning freezes the language model parameter while add a prefix
vector to each layer of the model. These prefix vectors work like
<code>virtual tokens</code> and are regarded as continuous prompts. By
only tuning these vectors, Prefix-Tuning outperforms finetuning in
low-data settings of NLG task.</p>
<p>Prefix-Tuning templates：</p>
<pre class="line-numbers language-none"><code class="language-none">Autoregressive Model: [T] x y
Encoder-Decoder Model: [T] x [T&#39;] y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>where x is the source table, y is the target utterance.</p>
<p>Notice that there is a comparison of prompting expressive power, it
seems that continuous vector is more expressive than discrete
templates:</p>
<blockquote>
<p>discrete prompting &lt; embedding-only ablation &lt;
prefix-tuning</p>
</blockquote>
<h3 id="p-tuning">P-Tuning</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.10385"># GPT Understands,
Too</a></p>
<p><img src="https://coriva.eu.org/images/nlp/ptuning.png"
alt="P-Tuning" /></p>
<p>The idea of P-Tuning is quite similar to Prefix-Tuning, both of them
try to training continuous prompts by labeled data. The difference is
that P-Tuning is mainly focusing on NLU tasks.</p>
<blockquote>
<p>To automatically search prompts in the continuous space to bridge the
gap between GPTs and NLU applications.</p>
</blockquote>
<p>The template of P-Tuning：</p>
<pre class="line-numbers language-none"><code class="language-none">[h(0)]...[h(i)]; e(x); [h(i+1)]...[h(m)]; e(y)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>where h(i) is the prompt, e() is the embedding function, x is input
tokens and y is the target.</p>
<p>In practice, both Prefix-Tuning and P-Tuning use reparameterize
techniques to solve the unstable optimization problem:</p>
<blockquote>
<p>Empirically, directly updating the P parameters leads to unstable
optimization and a slight drop in performance.</p>
<p>By: Prefix-Tuning</p>
</blockquote>
<blockquote>
<p>In the P-tuning we propose to also model the h(i) as a sequence using
a prompt encoder consists of a very lite neural network that can solve
the discreteness and association problems.</p>
<p>By: P-Tuning</p>
</blockquote>
<p>By the way, there were many explanations on why BERT is more suitable
for NLU and GPT is more suitable for NLG. Does this paper proves the
previous explanations are wrong? :-)</p>
<h3 id="prompt-tuning">Prompt Tuning</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.08691"># The Power of Scale for
Parameter-Efficient Prompt Tuning</a></p>
<p>This paper was published at EMNLP 2021. Compared with prefix-tuning
which inserts prefix vector to every Transformer layer, Prompt Tuning
uses a single prompt representation which is prepended to the embedding
input. Therefore, Prompt Tuning is more parameter-efficient. Some
interesting observations:</p>
<h4 id="how-to-init-prompt">How to Init Prompt?</h4>
<blockquote>
<p>Conceptually, our soft-prompt modulates the frozen network’s behavior
in the same way as text preceding the input, so it follows that a
word-like representation might serve as a good initialization spot.</p>
</blockquote>
<h4 id="how-to-choose-prompt-length">How to Choose Prompt Length?</h4>
<p>The short answer: as short as possible. Experiments show that more
than 20 tokens only yield marginal gains.</p>
<blockquote>
<p>The parameter cost of our method is EP, where E is the token
embedding dimension and P is the prompt length. The shorter the prompt,
the fewer new parameters must be tuned, so we aim to find a minimal
length<br />
that still performs well.</p>
</blockquote>
<p><img src="https://coriva.eu.org/images/nlp/promptparametercomp.png"
alt="Prompt Tuning" /></p>
<p>From the above comparison, Prompt Tuning uses less than 0.01%
parameters for most model sizes. The most interesting point of this
paper is that preprending limited number of parameters to the input
layer would be enough.</p>
<h1 id="summary">Summary</h1>
<p>Prompting is undoubtedly one of the most popular NLP directions.
Through constructing or searching proper templates, prompt learning has
created new SOTA for many NLP tasks.</p>
<p>However, prompting still faces many challenges:</p>
<blockquote>
<p>Most of the work takes manually-designed prompts—prompt engineering
is non-trivial since a small perturbation can significantly affect the
model’s performance, and creating a perfect prompt requires both
understanding of LMs' inner workings and trial-and-error.</p>
<p>By: <a target="_blank" rel="noopener" href="https://gaotianyu.xyz/prompting/"># Prompting: Better
Ways of Using Language Models for NLP Tasks</a></p>
</blockquote>
<p>In conclusion, prompting is a promising technique to release the
power of pretrained language models. Nice to see more excellent work in
this area.</p>
<h1 id="reference">Reference</h1>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/399295895">#
NLP新宠——浅谈Prompt的前世今生</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/395115779">#
近代自然语言处理技术发展的“第四范式”</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/NLG/" rel="tag"># NLG</a>
              <a href="/tags/NLU/" rel="tag"># NLU</a>
              <a href="/tags/Language-Model/" rel="tag"># Language Model</a>
              <a href="/tags/Prompt/" rel="tag"># Prompt</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/briefintrotoprompt/" rel="prev" title="NLP Prompt技术简介">
                  <i class="fa fa-chevron-left"></i> NLP Prompt技术简介
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/mongodbendlessretry.en/" rel="next" title="MongoDB Transaction BulkWrite Endless Retry">
                  MongoDB Transaction BulkWrite Endless Retry <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">finisky</span>
</div><div><span class="post-meta-item-icon"><i class="far fa-eye"></i></span><span><a href="https://finicounter.eu.org/" target="_blank">Total Views</a>:</span><span id="finicount_views" style="display:inline;padding-left:5px;"></span><div> <script async src="//finicounter.eu.org/finicounter.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/finisky" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"en","enable":true,"serverURL":"https://comments.finisky.eu.org","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":true,"pageSize":10,"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/briefintrotoprompt.en/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body>
</html>
